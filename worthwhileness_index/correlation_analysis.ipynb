{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os, json, time, pandas_profiling, warnings\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "#from docx import Document\n",
    "#from docx.shared import Inches\n",
    "\n",
    "#from mlxtend.frequent_patterns import apriori\n",
    "#from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style\"))\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "#distance plot - titles in plots\n",
    "rcParams['axes.titlepad'] = 45\n",
    "rcParams['font.size'] = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings - possible values:\n",
    "# complete\n",
    "# merged\n",
    "# no-outliers\n",
    "# merged-no-outliers\n",
    "# merged-no-outliers_quant_002\n",
    "analysis = 'merged-no-outliers_quant_001'   \n",
    "\n",
    "\n",
    "\n",
    "# if analysis == 'complete':\n",
    "#     legs = 'all_legs_final_ds_user_info.pkl'\n",
    "#     img_path = 'img/'\n",
    "#     report_name = 'Results_01.05_15.10.docx'\n",
    "\n",
    "# elif analysis == 'merged':\n",
    "#     legs = 'all_legs_merged_1.pkl'\n",
    "#     img_path = 'img_merged/'\n",
    "#     report_name = 'Results_01.05_15.10_merged.docx'\n",
    "    \n",
    "# # elif analysis == 'no-outliers':    \n",
    "# #     legs = 'all_legs_final_ds_user_info_no_outlier.pkl'\n",
    "# #     img_path = 'img_nooutliers/'\n",
    "# #     report_name = 'Results_01.05_30.07_nooutliers.docx'\n",
    "    \n",
    "# elif analysis == 'merged-no-outliers_quant_001':    \n",
    "legs = 'all_legs_merged_no_outlier_0.01.pkl'\n",
    "img_path = 'img_merged_nooutliers/'\n",
    "#report_name = 'Results_01.05_15.10_merged_nooutliers_0.01.docx'\n",
    "    \n",
    "# elif analysis == 'merged-no-outliers_quant_002':    \n",
    "#     legs = 'all_legs_merged_no_outlier_quant_002.pkl'\n",
    "#     img_path = 'img_merged-no-outliers_quant_002/'\n",
    "#     report_name = 'Results_01.05_30.07_merged-no-outliers_quant_002.docx'\n",
    "\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    os.makedirs(img_path)\n",
    "    \n",
    "\n",
    "#Global variables\n",
    "cutting_date = '2019-05-01' # remove trips and data published before this date\n",
    "meta_data_path = '../../data-campaigns/meta-data/'\n",
    "input_path = '../../out_2019.10.15/'\n",
    "report_path = '../reports/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "- `all_legs_final_ds_user_info`: all data about trips, legs and users\n",
    "- `trips_users_df`: match trip-user with date info\n",
    "- `trips_df`: original df with trip info\n",
    "- `values_from_trip`: for each leg the values for Productivity (paid work + personal tasks), Enjoyment, Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pre-processed datasets\n",
    "all_legs_final_ds_user_info = pd.read_pickle(input_path + legs)\n",
    "trips_users_df = pd.read_pickle(input_path + 'trips_users_df.pkl')\n",
    "# users_df_with_trips = pd.read_pickle(out_path + 'pre-processed_ds/users_df_with_trips.pkl')\n",
    "trips_df = pd.read_pickle(input_path+'trips_df_geoinfo.pkl')\n",
    "values_from_trip= pd.read_pickle(input_path + 'values_from_trip.pkl')\n",
    "print(values_from_trip.shape)\n",
    "values_from_trip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing on `values_from_trip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Available categories ['Paid_work', 'Personal_tasks', 'Enjoyment', 'Fitness', 'Unknown']\n",
    "# remove unknown from the categories \n",
    "tmp0 = values_from_trip[values_from_trip.valueFromTrip != 'Unknown']\n",
    "\n",
    "### Create a new df with this structure:\n",
    "# legid, Enjoyment, Fitness, Paid_work, Personal_tasks, wastedTime, Productivity\n",
    "\n",
    "# select only column we need\n",
    "tmp = tmp0[['legid', 'valueFromTrip', 'value']]\n",
    "# create pivot table with this columns: legid, E, F, Pw, Pt\n",
    "tmp2 = tmp.pivot(index='legid', columns='valueFromTrip', values= 'value').reset_index()\n",
    "# add also WT column \n",
    "tmp3 = pd.merge(tmp2, all_legs_final_ds_user_info[['legid', 'wastedTime']], on='legid', how='left')\n",
    "# remove rows with NAN in WT\n",
    "tmp4 = tmp3[tmp3.wastedTime.notna()]\n",
    "# select values of WT in [1,5]\n",
    "tmp5 = tmp4[tmp4.wastedTime.between(1,5)]\n",
    "# convert WT in numeric variable and make all values int\n",
    "tmp5.wastedTime = pd.to_numeric(tmp5.wastedTime)\n",
    "tmp5.wastedTime = np.round(tmp5.wastedTime)\n",
    "\n",
    "# merge Paid_work and Personal_tasks into Productivity\n",
    "# (!!) considering the MAXIMUM value\n",
    "tmp5['Productivity'] =tmp5[['Paid_work', 'Personal_tasks']].max(axis=1)\n",
    "\n",
    "values_from_trip2 = tmp5.copy()\n",
    "print('Final shape:', values_from_trip2.shape)\n",
    "values_from_trip2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "values_from_trip2.to_csv('values_from_trip2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= values_from_trip2[(values_from_trip2['Enjoyment']==0)&\n",
    "                 (values_from_trip2['Fitness']==0)&\n",
    "                 (values_from_trip2['Productivity']==0)].groupby('wastedTime').size().reset_index()\n",
    "test.columns = ['wastedTime','#leg000']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mord\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "X = values_from_trip2[['Enjoyment', 'Fitness', 'Productivity']]\n",
    "y = values_from_trip2['wastedTime']\n",
    "\n",
    "\n",
    "mul_lr = mord.OrdinalRidge(alpha=0.001,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    max_iter=None,\n",
    "    tol=0.001,\n",
    "    solver='auto').fit(X, y)\n",
    "\n",
    "mul_lr.coef_\n",
    "values_from_trip2['pred'] = mul_lr.predict(X)\n",
    "values_from_trip2[values_from_trip2['wastedTime'] == 1].head(10)\n",
    "values_from_trip2['pred'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Association analysis\n",
    "\n",
    "1. Distribution of all the variables\n",
    "2. Conditional distribution of PEF wrt WT\n",
    "3. Average of WT wrt PEF sum\n",
    "4. Chi-squared association and Cramer's V - each of PEF wrt WT\n",
    "5. Comparison on average WT versus PEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Distribution of all the variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Distribution of Wasted Time variable - relative and absolute frequencies\n",
    "tmp = pd.DataFrame(values_from_trip2.wastedTime.value_counts())\n",
    "tmp['rel_wastedTime'] = values_from_trip2.wastedTime.value_counts()/len(values_from_trip2)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## General distributions of variables\n",
    "# from matplotlib import rcParams\n",
    "# rcParams['axes.titlepad'] =5\n",
    "\n",
    "# fig, axs = plt.subplots(2,3, figsize=(15,7))\n",
    "# plt.subplots_adjust(top=1)\n",
    "\n",
    "# for idx,ax in list(enumerate(axs.flat)):\n",
    "#     print(idx)\n",
    "#     col_name = list(values_from_trip2.columns)[idx+1]\n",
    "# col_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## General distributions of variables\n",
    "from matplotlib import rcParams\n",
    "rcParams['axes.titlepad'] =5\n",
    "\n",
    "fig, axs = plt.subplots(2,3, figsize=(15,7))\n",
    "plt.subplots_adjust(top=1)\n",
    "\n",
    "for idx,ax in list(enumerate(axs.flat)):\n",
    "    col_name = list(values_from_trip2.columns)[idx+1]\n",
    "    weights = np.zeros_like(values_from_trip2.iloc[:,idx+1]) + 1. / len(values_from_trip2.iloc[:,idx+1])\n",
    "    ax.hist(values_from_trip2.iloc[:,idx+1], weights= weights)\n",
    "    ax.set_title(col_name)\n",
    "\n",
    "    ax.set_xticks(range(len(values_from_trip2.iloc[:,idx+1].unique())))\n",
    "    if col_name == 'wastedTime':\n",
    "        ax.set_xticks(range(1, len(values_from_trip2.iloc[:,idx+1].unique())+1))\n",
    "        ax.set_xlim(left=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conditional distribution of PEF wrt WT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_plot = sns.FacetGrid(data=values_from_trip2, col='wastedTime', sharey=False) #, hue='CentralAir', col_wrap=4)\n",
    "cond_plot.map(plt.hist, 'Enjoyment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_plot = sns.FacetGrid(data=values_from_trip2, col='wastedTime', sharey=False)\n",
    "cond_plot.map(plt.hist, 'Fitness');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_plot = sns.FacetGrid(data=values_from_trip2, col='wastedTime', sharey=False)\n",
    "cond_plot.map(plt.hist, 'Productivity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Average of WT wrt PEF sum***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the sum\n",
    "values_from_trip2['PEF'] = values_from_trip2[['Enjoyment', 'Fitness', 'Productivity']].sum(axis=1)\n",
    "# select only columns we need, group by PEF sum and make the mean of WT\n",
    "pef_sum = values_from_trip2[['legid', 'PEF', 'wastedTime']].groupby('PEF').mean()\n",
    "pef_sum\n",
    "\n",
    "### Interpretation: legs with sum of Enjoyment, Fitness and Productivity equal to 0 \n",
    "# have 3 as wastedTime *on average*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Chi-squared association and Cramer's V***\n",
    "\n",
    "Evaluate the association between:\n",
    "- Enjoyment and wastedTime\n",
    "- Fitness and wastedTime\n",
    "- Productivity and wastedTime\n",
    "\n",
    "Ref: https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V\n",
    "\n",
    "Cramer's V:\n",
    "- 0: no association\n",
    "- 1: complete association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramer_v(tab):\n",
    "    chi2 = chi2_contingency(tab)[0]\n",
    "    n = sum(tab.sum())\n",
    "    phi2 = chi2/n\n",
    "    r,k = tab.shape\n",
    "\n",
    "    return(np.sqrt(phi2 / min( (k-1), (r-1))))\n",
    "\n",
    "CV_enj = cramer_v(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Enjoyment))\n",
    "CV_fit = cramer_v(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Fitness))\n",
    "CV_pro = cramer_v(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Productivity))\n",
    "print(\"Cramer's V\")\n",
    "print('E:', CV_enj, ' - F:', CV_fit, ' - P:', CV_pro)\n",
    "print()\n",
    "\n",
    "print('chi squared test')\n",
    "print('E:', chi2_contingency(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Enjoyment))[1], \n",
    "      ' - F:', chi2_contingency(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Fitness))[1], \n",
    "      ' - P:', chi2_contingency(pd.crosstab(values_from_trip2.wastedTime, values_from_trip2.Productivity))[1])\n",
    "\n",
    "### Interpretation:\n",
    "# There is 30% of association between Enjoyment and wastedTime \n",
    "\n",
    "## Chi-Squared\n",
    "# H0: distributions are significantly different\n",
    "# H1: distributions are not significantly different\n",
    "# with the chi squared test we have to reject the null hypothesis\n",
    "# distributions are not significantly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comparison on average WT versus PEF***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_from_trip2.pivot_table(index='wastedTime',  values='Enjoyment', aggfunc='mean')\n",
    "# legs with wastedTime equal to 1,2 have *on average* 0 for Enjoyment\n",
    "# legs with wastedTime equal to 3,4,5 have *on average* 1 for Enjoyment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_from_trip2.pivot_table(index='wastedTime',  values='Fitness', aggfunc='mean')\n",
    "# legs with wastedTime equal to 1,2,3 have *on average* 0 for Fitness\n",
    "# legs with wastedTime equal to 4,5 have *on average* 1 for Fitness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.round(values_from_trip2.pivot_table(index='wastedTime',  values='Productivity', aggfunc='mean'))\n",
    "# legs with wastedTime equal to 1,2 have *on average* 0 for Productivity\n",
    "# legs with wastedTime equal to 3,4,5 have *on average* 1 for Productivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_from_trip2[['Enjoyment', 'Fitness', 'Productivity', 'wastedTime']].groupby('wastedTime').mean()\n",
    "# legs with wastedTime equal to 1 have *on average* 0 for PEF\n",
    "# legs with wastedTime equal to 2 have *on average* 0 for PEF\n",
    "# legs with wastedTime equal to 3 have *on average* 0 for F and 1 for PE\n",
    "# legs with wastedTime equal to 4 have *on average* 1 for PEF\n",
    "# legs with wastedTime equal to 5 have *on average* 1 for PEF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: Walking dataset\n",
    "\n",
    "Considering only legs with `transp_category` equal to `walking` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transp_cat = 'Walking'\n",
    "x = all_legs_final_ds_user_info[['legid', 'transp_category']]\n",
    "trasnp = pd.merge(values_from_trip2, x, on='legid', how='left')\n",
    "print(trasnp.transp_category.unique())\n",
    "trasnp = trasnp[trasnp.transp_category == transp_cat]\n",
    "trasnp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trasnp[['Enjoyment', 'Fitness', 'Productivity', 'wastedTime']].melt('wastedTime', var_name='element', value_name='Val')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['wastedTime','element','Val']).size().reset_index()\n",
    "df1.columns = ['wastedTime','element','Val','freq']\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1,5, figsize=(15,7))\n",
    "# plt.subplots_adjust(top=1)\n",
    "\n",
    "for idx,ax in list(enumerate(axs.flat)): \n",
    "    plt.subplot(1, 5, idx+1)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    sns.barplot(data = df1[df1['wastedTime']==idx+1], x=\"element\", y='freq', hue='Val').set(\n",
    "    xlabel='wastedTime', \n",
    "    ylabel = 'Freq' )\n",
    "    \n",
    "    plt.title('WastedTime ' + str(idx+1), y=1.)\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['wastedTime']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond_plot = sns.FacetGrid(data=df1, col='wastedTime', hue='element', sharey=False) #, hue='CentralAir', col_wrap=4)\n",
    "# cond_plot.map(sns.barplot,  \"Val\", 'freq').add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond_plot = sns.FacetGrid(data=trasnp, col='wastedTime', sharey=False) #, hue='CentralAir', col_wrap=4)\n",
    "# cond_plot.map(plt.hist, 'Fitness');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cond_plot = sns.FacetGrid(data=trasnp, col='wastedTime', sharey=False) #, hue='CentralAir', col_wrap=4)\n",
    "# cond_plot.map(plt.hist, 'Productivity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trasnp[['Enjoyment', 'Fitness', 'Productivity', 'wastedTime']].groupby('wastedTime').mean()\n",
    "# legs with wastedTime equal to 1 have *on average* 0 for PEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mord\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "X = trasnp[['Enjoyment', 'Fitness', 'Productivity']]\n",
    "y = trasnp['wastedTime']\n",
    "\n",
    "\n",
    "mul_lr = mord.OrdinalRidge(alpha=0.001,\n",
    "    fit_intercept=True,\n",
    "    normalize=False,\n",
    "    copy_X=True,\n",
    "    max_iter=None,\n",
    "    tol=0.001,\n",
    "    solver='auto').fit(X, y)\n",
    "\n",
    "print('Coeficinets: ', mul_lr.coef_)\n",
    "trasnp['pred'] = mul_lr.predict(X)\n",
    "trasnp[trasnp['wastedTime'] == 1].head(10)\n",
    "trasnp['pred'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_legs_final_ds_user_info[['legid', 'transp_category']]\n",
    "df_0 = pd.merge(values_from_trip2, x, on='legid', how='left')\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = df_0[(df_0['Enjoyment'] == 0) & (df_0['Fitness'] == 0) & (df_0['Productivity'] == 0)]\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0.groupby('wastedTime').size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
