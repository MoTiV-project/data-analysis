{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymization notebook\n",
    "\n",
    "## Steps\n",
    "\n",
    "- GPS coordinates anonymization\n",
    "    - Read legs coords\n",
    "    - Read shapefile\n",
    "    - add geometry to End poin\n",
    "\n",
    "- Coordinate Anonymizazion Procedure\n",
    "- Conversion from pkl to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "## system libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pathlib\n",
    "from glob import glob\n",
    "from datetime import date, datetime\n",
    "\n",
    "## numerical libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "## plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# geo libraries\n",
    "import geopandas as gpd\n",
    "\n",
    "# reports\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebool options\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "rcParams[\"axes.titlepad\"] = 45\n",
    "rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "CUTTING_DATE = \"2019-05-01\"  # remove trips and data published before this date\n",
    "\n",
    "data_campaigns_path = os.path.join(\"../..\", \"data-campaigns/\")\n",
    "meta_data_path = os.path.join(data_campaigns_path, \"meta-data/\")\n",
    "shape_data = os.path.join(data_campaigns_path, \"shapefiles/\")\n",
    "gps_data = os.path.join(data_campaigns_path, \"2020-01-15.GPS/\")\n",
    "out_path = pathlib.Path(\"../../2019-12-16.out/\")\n",
    "out_path_dataset = pathlib.Path(\"../../2019-12-16.out/dataset/\")\n",
    "\n",
    "# preprocessed data\n",
    "input_path = os.path.join(\"../..\", \"2019-12-16.out/\")\n",
    "\n",
    "# raw input data\n",
    "raw_data_path = os.path.join(data_campaigns_path, \"2019-10-30.all/\")\n",
    "raw_data_update_path = os.path.join(data_campaigns_path, \"2019-12-16.update/\")\n",
    "\n",
    "# anon dataset (output)\n",
    "anon_dataset_dir = input_path.rstrip(\"/\").split(\"/\")[-1].replace(\".out\", \".anon\")\n",
    "anon_dataset_path = os.path.join(\"../..\", \"anon-dataset\", anon_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.abspath(anon_dataset_path))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(anon_dataset_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.abspath(out_path_dataset))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(out_path_dataset), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_coords(value, lat, lon):\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "legs = \"all_legs_merged_no_outlier_0.01.pkl\"\n",
    "trips_users = \"trips_users_df.pkl\"\n",
    "trips = \"trips_df.pkl\"\n",
    "\n",
    "# read datasets\n",
    "legs_df = pd.read_pickle(input_path + legs)\n",
    "trips_users_df = pd.read_pickle(input_path + trips_users)\n",
    "trips_df = pd.read_pickle(input_path + trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "legs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trips_users_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS coordinates anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read legs gps coorinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read legs data\n",
    "gps_data_df = pd.read_csv(gps_data + \"allLegs.csv\")\n",
    "gps_data_df.rename(columns={\"legId\": \"legid\"}, inplace=True)\n",
    "\n",
    "gps_data_df.drop_duplicates([\"legid\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gps_data_df.shape)\n",
    "gps_data_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = gpd.read_file(shape_data + \"Italy/Italy.shp\")\n",
    "xxx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = gpd.read_file(shape_data + \"Italy/Italy_core.shp\")\n",
    "xxx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "def read_shapes_in_dir_with_ext(directory_str, extension_str=\".shp\", quiet=False):\n",
    "\n",
    "    shape_path = os.path.join(directory_str, \"*{}\".format(extension_str))\n",
    "    shape_files = glob(shape_path)\n",
    "\n",
    "    df_list = []\n",
    "    for shape_file in shape_files:\n",
    "        filename = shape_file.rstrip(\"/\").split(\"/\")[-1]\n",
    "        if not quiet:\n",
    "            print(\"  - reading {}...\".format(filename))\n",
    "\n",
    "        tmp_df = gpd.read_file(shape_file)\n",
    "        df_list.append(tmp_df)\n",
    "\n",
    "    final_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def read_shapes(extension_str=\".shp\", quiet=False):\n",
    "    shape_df_list = list()\n",
    "    shape_dirs = glob(os.path.join(shape_data, \"*/\"))\n",
    "\n",
    "    for country_dir in shape_dirs:\n",
    "        country = country_dir.rstrip(\"/\").split(\"/\")[-1]\n",
    "        if not quiet:\n",
    "            print(\"* reading shapefiles for {}...\".format(country))\n",
    "        shape_df_list.append(\n",
    "            read_shapes_in_dir_with_ext(country_dir, extension_str, quiet)\n",
    "        )\n",
    "\n",
    "    shapes_df_all = pd.concat(shape_df_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return shapes_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all = read_shapes(\".shp\")\n",
    "print(shapes_df_all.shape)\n",
    "shapes_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all = read_shapes(\"*_core.shp\")\n",
    "print(shapes_df_all.shape)\n",
    "shapes_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "shapes_df_all = read_shapes(\".shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_start_point(gps_data_df, classe, level):\n",
    "geom = gps_data_df.apply(lambda x: Point([x[\"StartLon\"], x[\"StartLat\"]]), axis=1)\n",
    "gps_data_df = gpd.GeoDataFrame(gps_data_df, geometry=geom)\n",
    "gps_data_df.crs = {\"init\": \"epsg:4326\"}\n",
    "gps_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\"gps_data_df.shape:\", gps_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# If you get an error of\n",
    "#    sjoin 'NoneType' object has no attribute 'intersection'\n",
    "# you need to check if yoiu have the rtree package installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_start = gpd.sjoin(gps_data_df, shapes_df_all, how=\"left\", op=\"intersects\")\n",
    "print(gps_cities_start.shape)\n",
    "gps_cities_start.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "gps_cities_start.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_start\",\n",
    "        \"iso3\": \"country_start\",\n",
    "        \"class_code\": \"class_code_start\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "gps_cities_start = gps_cities_start[\n",
    "    [\n",
    "        \"legid\",\n",
    "        \"StartLat\",\n",
    "        \"StartLon\",\n",
    "        \"EndLat\",\n",
    "        \"EndLon\",\n",
    "        \"fuaname_en_start\",\n",
    "        \"country_start\",\n",
    "        \"class_code_start\",\n",
    "    ]\n",
    "]\n",
    "gps_cities_start.head(3)\n",
    "gps_cities_start[\"start_class\"] = gps_cities_start.apply(\n",
    "    lambda x: \"-\" if pd.isnull(x[\"fuaname_en_start\"]) else \"sub-urban\", axis=1\n",
    ")\n",
    "gps_cities_start.head()\n",
    "\n",
    "shapes_df_all = read_shapes(\"_core.shp\")\n",
    "geom = gps_cities_start.apply(lambda x: Point([x[\"EndLon\"], x[\"EndLat\"]]), axis=1)\n",
    "gps_cities_start = gpd.GeoDataFrame(gps_cities_start, geometry=geom)\n",
    "gps_cities_start.crs = {\"init\": \"epsg:4326\"}\n",
    "gps_cities_start.head()\n",
    "\n",
    "print(\"gps_data_df.shape:\", gps_data_df.shape)\n",
    "gps_cities_start = gpd.sjoin(\n",
    "    gps_cities_start, shapes_df_all, how=\"left\", op=\"intersects\"\n",
    ")\n",
    "print(\"gps_cities_start.shape:\", gps_cities_start.shape)\n",
    "gps_cities_start.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_start.groupby(\"start_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_start.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_start2\",\n",
    "        \"iso3\": \"country_start2\",\n",
    "        \"class_code\": \"class_code_start2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "gps_cities_start = gps_cities_start[\n",
    "    [\n",
    "        \"legid\",\n",
    "        \"StartLat\",\n",
    "        \"StartLon\",\n",
    "        \"EndLat\",\n",
    "        \"EndLon\",\n",
    "        \"fuaname_en_start\",\n",
    "        \"country_start\",\n",
    "        \"fuaname_en_start2\",\n",
    "        \"country_start2\",\n",
    "        \"class_code_start2\",\n",
    "        \"start_class\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "gps_cities_start[\"start_class\"] = gps_cities_start.apply(\n",
    "    lambda x: x[\"start_class\"] if pd.isnull(x[\"fuaname_en_start2\"]) else \"urban\", axis=1\n",
    ")\n",
    "gps_cities_start[\"start_class\"] = gps_cities_start[\"start_class\"].replace(\"-\", \"rural\")\n",
    "\n",
    "gps_cities_start[\"country_start\"] = gps_cities_start.apply(\n",
    "    lambda x: x[\"country_start\"]\n",
    "    if pd.isnull(x[\"country_start2\"])\n",
    "    else x[\"country_start2\"],\n",
    "    axis=1,\n",
    ")\n",
    "gps_cities_start[\"fuaname_en_start\"] = gps_cities_start.apply(\n",
    "    lambda x: x[\"fuaname_en_start\"]\n",
    "    if pd.isnull(x[\"fuaname_en_start2\"])\n",
    "    else x[\"fuaname_en_start2\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "gps_cities_start.head(3)\n",
    "\n",
    "\n",
    "# gps_cities_start['start_class'] = gps_cities_start.apply(lambda x: x['start_class'] if pd.isnull(x['fuaname_en_start']) else classe , axis=1)\n",
    "# gps_cities_start.head()\n",
    "# return gps_cities_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_start.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gps_cities_start.groupby(\"start_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add geometry to End point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "shapes_df_all = read_shapes(\".shp\")\n",
    "\n",
    "# def classify_end_point(gps_data_df, classe, level):\n",
    "geom = gps_data_df.apply(lambda x: Point([x[\"EndLon\"], x[\"EndLat\"]]), axis=1)\n",
    "gps_data_df = gpd.GeoDataFrame(gps_data_df, geometry=geom)\n",
    "gps_data_df.crs = {\"init\": \"epsg:4326\"}\n",
    "gps_data_df.head()\n",
    "\n",
    "\n",
    "print(gps_data_df.shape)\n",
    "gps_cities_end = gpd.sjoin(gps_data_df, shapes_df_all, how=\"left\", op=\"intersects\")\n",
    "print(gps_cities_end.shape)\n",
    "gps_cities_end.head(3)\n",
    "\n",
    "\n",
    "gps_cities_end.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_end\",\n",
    "        \"iso3\": \"country_end\",\n",
    "        \"class_code\": \"class_code_end\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "gps_cities_end = gps_cities_end[\n",
    "    [\n",
    "        \"legid\",\n",
    "        \"StartLat\",\n",
    "        \"StartLon\",\n",
    "        \"EndLat\",\n",
    "        \"EndLon\",\n",
    "        \"fuaname_en_end\",\n",
    "        \"country_end\",\n",
    "        \"class_code_end\",\n",
    "    ]\n",
    "]\n",
    "gps_cities_end.head(3)\n",
    "gps_cities_end[\"end_class\"] = gps_cities_end.apply(\n",
    "    lambda x: \"-\" if pd.isnull(x[\"fuaname_en_end\"]) else \"sub-urban\", axis=1\n",
    ")\n",
    "gps_cities_end.head()\n",
    "\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "\n",
    "shapes_df_all = read_shapes(\"_core.shp\")\n",
    "geom = gps_cities_end.apply(lambda x: Point([x[\"EndLon\"], x[\"EndLat\"]]), axis=1)\n",
    "gps_cities_end = gpd.GeoDataFrame(gps_cities_end, geometry=geom)\n",
    "gps_cities_end.crs = {\"init\": \"epsg:4326\"}\n",
    "gps_cities_end.head()\n",
    "\n",
    "print(gps_data_df.shape)\n",
    "gps_cities_end = gpd.sjoin(gps_cities_end, shapes_df_all, how=\"left\", op=\"intersects\")\n",
    "print(gps_cities_end.shape)\n",
    "gps_cities_end.head(3)\n",
    "\n",
    "gps_cities_end.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_end2\",\n",
    "        \"iso3\": \"country_end2\",\n",
    "        \"class_code\": \"class_code_end2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "gps_cities_end = gps_cities_end[\n",
    "    [\n",
    "        \"legid\",\n",
    "        \"StartLat\",\n",
    "        \"StartLon\",\n",
    "        \"EndLat\",\n",
    "        \"EndLon\",\n",
    "        \"fuaname_en_end\",\n",
    "        \"country_end\",\n",
    "        \"fuaname_en_end2\",\n",
    "        \"country_end2\",\n",
    "        \"class_code_end2\",\n",
    "        \"end_class\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "gps_cities_end[\"end_class\"] = gps_cities_end.apply(\n",
    "    lambda x: x[\"end_class\"] if pd.isnull(x[\"fuaname_en_end2\"]) else \"urban\", axis=1\n",
    ")\n",
    "gps_cities_end[\"end_class\"] = gps_cities_end[\"end_class\"].replace(\"-\", \"rural\")\n",
    "\n",
    "gps_cities_end[\"country_end\"] = gps_cities_end.apply(\n",
    "    lambda x: x[\"country_end\"] if pd.isnull(x[\"country_end2\"]) else x[\"country_end2\"],\n",
    "    axis=1,\n",
    ")\n",
    "gps_cities_end[\"fuaname_en_end\"] = gps_cities_end.apply(\n",
    "    lambda x: x[\"fuaname_en_end\"]\n",
    "    if pd.isnull(x[\"fuaname_en_end2\"])\n",
    "    else x[\"fuaname_en_end2\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "gps_cities_end.head(3)\n",
    "\n",
    "\n",
    "# gps_cities_end['end_class'] = gps_cities_end.apply(lambda x: x['end_class'] if pd.isnull(x['fuaname_en_end']) else classe , axis=1)\n",
    "# gps_cities_end.head()\n",
    "# return gps_cities_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tmp_shape = shapes_df_all[\"geometry\"].iloc[0]\n",
    "print(\"Centroid:\", tmp_shape.centroid)\n",
    "print(\"(x, y):\", \"({}, {})\".format(tmp_shape.centroid.x, tmp_shape.centroid.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all[\"centroid_x\"] = shapes_df_all[\"geometry\"].apply(\n",
    "    lambda shape: shape.centroid.x\n",
    ")\n",
    "shapes_df_all[\"centroid_y\"] = shapes_df_all[\"geometry\"].apply(\n",
    "    lambda shape: shape.centroid.y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_end.groupby(\"end_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities = pd.merge(\n",
    "    gps_cities_start,\n",
    "    gps_cities_end[\n",
    "        [\"legid\", \"fuaname_en_end\", \"country_end\", \"class_code_end2\", \"end_class\"]\n",
    "    ],\n",
    "    on=\"legid\",\n",
    ")\n",
    "print(gps_cities.shape)\n",
    "gps_cities.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save coordinate pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities.to_pickle(input_path + \"gps_cities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_df_all.to_pickle(input_path + \"shapes_df_all.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add urban/sub-urban/rural classification to legs DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "legs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(legs_df.shape)\n",
    "all_legs_final_ds_user_info_gps = pd.merge(legs_df, gps_data_df, on=\"legid\", how=\"left\")\n",
    "print(all_legs_final_ds_user_info_gps.shape)\n",
    "print(\n",
    "    \"- Total missing gps coordinates: \",\n",
    "    all_legs_final_ds_user_info_gps[\n",
    "        all_legs_final_ds_user_info_gps.EndLat.isna()\n",
    "    ].shape[0],\n",
    ")\n",
    "print(\n",
    "    \"- Of the above missing coordinates, the number of transfer leg (waitingEvent) is :\",\n",
    "    all_legs_final_ds_user_info_gps[\n",
    "        all_legs_final_ds_user_info_gps[\"class\"] == \"WaitingEvent\"\n",
    "    ].shape[0],\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"- While, the number of leg (class=Leg) is with missing coordinates is :\",\n",
    "    all_legs_final_ds_user_info_gps[\n",
    "        (all_legs_final_ds_user_info_gps.EndLat.isna())\n",
    "        & (all_legs_final_ds_user_info_gps[\"class\"] == \"Leg\")\n",
    "    ].shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info_urban_class = pd.merge(\n",
    "    legs_df,\n",
    "    gps_cities[\n",
    "        [\n",
    "            \"legid\",\n",
    "            \"fuaname_en_start\",\n",
    "            \"country_start\",\n",
    "            \"fuaname_en_end\",\n",
    "            \"country_end\",\n",
    "            \"start_class\",\n",
    "            \"end_class\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"legid\",\n",
    "    how=\"left\",\n",
    ")\n",
    "all_legs_final_ds_user_info_urban_class.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info_urban_class[\n",
    "    all_legs_final_ds_user_info_urban_class[\"start_class\"].isna()\n",
    "].shape\n",
    "all_legs_final_ds_user_info_urban_class.to_pickle(\n",
    "    input_path + \"all_legs_final_ds_user_info_urban_class.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_dist = (\n",
    "    all_legs_final_ds_user_info_urban_class.groupby([\"start_class\", \"end_class\"])\n",
    "    .size()\n",
    "    .reset_index()\n",
    ")\n",
    "zone_dist.columns = [\"start_class\", \"end_class\", \"#legs\"]\n",
    "print(zone_dist[\"#legs\"].sum())\n",
    "zone_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66432+21378 (missing coordinates , 21228 transfer legs and 150 legs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(\n",
    "    all_legs_final_ds_user_info_urban_class[\n",
    "        ~all_legs_final_ds_user_info_urban_class[\"start_class\"].isna()\n",
    "    ][\"tripid\"].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_legs_final_ds_user_info_urban_class[\"userid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_users_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info_urban_class.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinate Anonymizazion Procedure\n",
    "\n",
    "* city area round to 50-100m (better more than less ie 100m): 3rd decimal place\n",
    "* suburb - commuting zone - 250m - round third decimal to .5\n",
    "* rural - 500m - round 2nd decimal to .5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_digit(num, digit=5):\n",
    "    return digit * round(num / digit)\n",
    "\n",
    "\n",
    "def round_to_decimal_digit(num, ndecimals=0, digit=5):\n",
    "    rounded = round(num, ndecimals) * pow(10, ndecimals)\n",
    "    return round_to_digit(rounded, digit) / float(pow(10, ndecimals))\n",
    "\n",
    "\n",
    "def anonymize_coord(coord, coord_class):\n",
    "    anon_coord = -1\n",
    "\n",
    "    # TODO if coord is not a number?\n",
    "\n",
    "    if coord_class == \"urban\":\n",
    "        anon_coord = round(coord, 3)\n",
    "    elif coord_class == \"sub-urban\":\n",
    "        anon_coord = round_to_decimal_digit(coord, 3, 5)\n",
    "    else:\n",
    "        anon_coord = round_to_decimal_digit(coord, 2, 5)\n",
    "\n",
    "    return anon_coord\n",
    "\n",
    "\n",
    "def anonymize_coord_nan(coord, coord_class):\n",
    "    if pd.isna(coord):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return anonymize_coord(coord, coord_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info_urban_class.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anonymize_coord(38.635434, \"sub-urban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [0.173257, 0.17713, 0.17813, 0.17432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coord in coords:\n",
    "    print(coord)\n",
    "    print(\"urban:\", anonymize_coord(coord, \"urban\"))\n",
    "    print(\"sub-urban:\", anonymize_coord(coord, \"sub-urban\"))\n",
    "    print(\"rural:\", anonymize_coord(coord, \"rural\"))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info_urban_class.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_cities.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_gps = gps_cities.copy()\n",
    "\n",
    "anon_gps[\"StartLat\"] = anon_gps.apply(\n",
    "    lambda row: anonymize_coord(row[\"StartLat\"], row[\"start_class\"]), axis=1\n",
    ")\n",
    "anon_gps[\"StartLon\"] = anon_gps.apply(\n",
    "    lambda row: anonymize_coord(row[\"StartLon\"], row[\"start_class\"]), axis=1\n",
    ")\n",
    "\n",
    "anon_gps[\"EndLat\"] = anon_gps.apply(\n",
    "    lambda row: anonymize_coord(row[\"EndLat\"], row[\"end_class\"]), axis=1\n",
    ")\n",
    "anon_gps[\"EndLon\"] = anon_gps.apply(\n",
    "    lambda row: anonymize_coord(row[\"EndLon\"], row[\"end_class\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_gps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstodrop = [\n",
    "    \"fuaname_en_start2\",\n",
    "    \"country_start2\",\n",
    "    \"class_code_start2\",\n",
    "    \"class_code_end2\",\n",
    "]\n",
    "anon_gps.drop(colstodrop, axis=1, inplace=True)\n",
    "\n",
    "colstorename = {\n",
    "    \"StartLat\": \"start_lat\",\n",
    "    \"StartLon\": \"start_lon\",\n",
    "    \"EndLat\": \"end_lat\",\n",
    "    \"EndLon\": \"end_lon\",\n",
    "    \"fuaname_en_start\": \"start_name\",\n",
    "    \"country_start\": \"start_country\",\n",
    "    \"fuaname_en_end\": \"end_name\",\n",
    "    \"country_end\": \"end_country\",\n",
    "}\n",
    "\n",
    "anon_gps.rename(columns=colstorename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_gps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"anon coordinates DF shape: \", anon_gps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_gps.drop_duplicates(subset=[\"legid\"], keep=\"first\", inplace=True)\n",
    "print(\"anon coordinates DF shape: \", anon_gps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"legs_coordinates.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "\n",
    "anon_gps.to_csv(output_path, index=False, header=True, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_legs: Conversion from pkl to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all legs preprocessed: \", legs_df.shape)\n",
    "legs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_remove = [\n",
    "    \"workAddress._id\",\n",
    "    \"workAddress.address\",\n",
    "    \"homeAddress._id\",\n",
    "    \"homeAddress.address\",\n",
    "]\n",
    "legs_df_anon = legs_df.drop(cols_to_remove, axis=1)\n",
    "print(legs_df_anon.shape)\n",
    "\n",
    "## save to csv\n",
    "output_file = \"all_legs_merged_no_outlier_0.01_anonymized.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "legs_df_anon.to_csv(output_path, index=False, header=True, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. All trips until 30-10-2019\n",
    "\n",
    "trips_prev_df = []\n",
    "first = True\n",
    "read_files = glob(raw_data_path + \"*_tripsData.json\")\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips = json.loads(f.read())\n",
    "        trips_prev_df_temp = json_normalize(trips)\n",
    "    print(trips_prev_df_temp.shape)\n",
    "    tot += trips_prev_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        trips_prev_df = trips_prev_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        trips_prev_df = pd.concat([trips_prev_df, trips_prev_df_temp])\n",
    "\n",
    "print()\n",
    "print(\"Tot = \", tot)\n",
    "print(trips_prev_df.shape)\n",
    "print()\n",
    "print(\"Remove duplicates...\")\n",
    "trips_prev_df_nodup = trips_prev_df.drop_duplicates([\"tripid\"], keep=\"first\")\n",
    "print(trips_prev_df_nodup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Updated trips\n",
    "\n",
    "trips_updated_df = []\n",
    "first = True\n",
    "read_files = glob(raw_data_update_path + \"*_tripsData.json\")\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips = json.loads(f.read())\n",
    "        trips_updated_df_temp = json_normalize(trips)\n",
    "    print(trips_updated_df_temp.shape)\n",
    "    tot += trips_updated_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        trips_updated_df = trips_updated_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        trips_updated_df = pd.concat([trips_updated_df, trips_updated_df_temp])\n",
    "\n",
    "print()\n",
    "print(\"Tot = \", tot)\n",
    "print(trips_updated_df.shape)\n",
    "print()\n",
    "print(\"Remove duplicates...\")\n",
    "trips_updated_df_nodup = trips_updated_df.drop_duplicates([\"tripid\"], keep=\"first\")\n",
    "print(trips_updated_df_nodup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "print(\"trips DF shape: {}\".format(trips_prev_df_nodup.shape))\n",
    "print(\"trips_update DF shape: {}\".format(trips_updated_df_nodup.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Merge all + updated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## 3. Merge all + updated\n",
    "trips_df_all = pd.concat([trips_prev_df_nodup, trips_updated_df_nodup])\n",
    "print(\"trips DF shape: {}\".format(trips_df_all.shape))\n",
    "\n",
    "trips_df_all[\"tripStartDate_formated\"] = pd.to_datetime(\n",
    "    trips_df_all[\"tripStartDate\"], unit=\"ms\"\n",
    ")\n",
    "trips_df_all[\"tripEndDate_formated\"] = pd.to_datetime(\n",
    "    trips_df_all[\"tripEndDate\"], unit=\"ms\"\n",
    ")\n",
    "\n",
    "# print('Remove duplicates...')\n",
    "# trips_df_nodup = trips_df.iloc[trips_df.astype(str).drop_duplicates(keep='first').index]\n",
    "# print('trips DF shape: {}'.format(trips_df.shape))\n",
    "print()\n",
    "print(\"Total trips: \", trips_df_all.shape[0])\n",
    "print(\"Total unique trips: \", len(trips_df_all[\"tripid\"].unique()))\n",
    "\n",
    "trips_df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "colstodrop = [\n",
    "    \"finalAddress\",\n",
    "    \"startAddress\",\n",
    "]\n",
    "trips_df_all.drop(colstodrop, axis=1, inplace=True)\n",
    "trips_df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips_df_all.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to JSON\n",
    "output_file = \"all_tripsData.json\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "trips_df_all.to_json(output_path, orient=\"split\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips = pd.read_pickle(input_path + \"users_df_with_trips.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_columns = [\n",
    "    col for col in users_df_with_trips.columns if \"address\" in col.lower()\n",
    "]\n",
    "print(\"address_columns:\", address_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_addresses = users_df_with_trips[\n",
    "    [\"userid\", \"city\"] + address_columns + [\"onCampaigns\"]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_addresses.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "LOCATOR = Nominatim(user_agent=\"Nominatim\")\n",
    "\n",
    "# 1 - conveneint function to delay between geocoding calls\n",
    "GEOCODER = RateLimiter(LOCATOR.geocode, min_delay_seconds=2, max_retries=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode(data, in_field, out_suffix):\n",
    "    # 2- - create location column\n",
    "\n",
    "    out_location = \"location_{}\".format(out_suffix)\n",
    "    data[out_location] = data[in_field].apply(GEOCODER)\n",
    "\n",
    "    # 3 - create longitude, laatitude and altitude from location column (returns tuple)\n",
    "    in_location = \"location_{}\".format(out_suffix)\n",
    "    out_point = \"point_{}\".format(out_suffix)\n",
    "    data[out_point] = data[in_location].apply(\n",
    "        lambda loc: tuple(loc.point) if loc else None\n",
    "    )\n",
    "\n",
    "    # 4 - split point column into latitude, longitude and altitude columns\n",
    "    in_point = \"point_{}\".format(out_suffix)\n",
    "    out_fields = [\n",
    "        field.format(out_suffix)\n",
    "        for field in [\"latitude_{}\", \"longitude_{}\", \"altitude_{}\"]\n",
    "    ]\n",
    "    data[out_fields] = pd.DataFrame(data[in_point].tolist(), index=data.index)\n",
    "\n",
    "    # replace None with NaN\n",
    "    data[out_location].fillna(value=np.nan, inplace=True)\n",
    "    data[out_point].fillna(value=np.nan, inplace=True)\n",
    "    data[out_fields].fillna(value=np.nan, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = [\n",
    "    adr for adr in list(user_addresses[\"workAddress.address\"].dropna().unique()) if adr\n",
    "]\n",
    "addresses_sample = random.sample(addresses, 5)\n",
    "\n",
    "for adr in addresses_sample:\n",
    "    print(\"Address: {} - \".format(adr), end=\"\")\n",
    "    location = LOCATOR.geocode(adr)\n",
    "    if location is not None:\n",
    "        print(\n",
    "            \"Latitude = {}, Longitude = {}\".format(\n",
    "                location.latitude, location.longitude\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode(data, in_field, out_field_suffix)\n",
    "geocoded_addresses = geocode(user_addresses, \"workAddress.address\", \"work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_addresses.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geocode(data, in_field, out_field_suffix)\n",
    "geocoded_addresses = geocode(geocoded_addresses, \"homeAddress.address\", \"home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geocoded_addresses.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = out_path / \"geocoded_addresses.pkl\"\n",
    "geocoded_addresses.to_pickle(out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User coordinates classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_addresses = pd.read_pickle(input_path + \"geocoded_addresses.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_addresses.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_addresses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "shapes_df_all = read_shapes(\".shp\", quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_work_coords = geocoded_addresses[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"longitude_work\",\n",
    "        \"latitude_work\",\n",
    "    ]\n",
    "].copy()\n",
    "geocoded_work_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "shapes_df_all = read_shapes(\".shp\", quiet=True)\n",
    "\n",
    "# def classify_end_point(geocoded_work_coords, classe, level):\n",
    "geom = geocoded_work_coords.apply(\n",
    "    lambda x: Point([x[\"longitude_work\"], x[\"latitude_work\"]]), axis=1\n",
    ")\n",
    "geocoded_work_coords = gpd.GeoDataFrame(geocoded_work_coords, geometry=geom)\n",
    "geocoded_work_coords.crs = {\"init\": \"epsg:4326\"}\n",
    "geocoded_work_coords.head()\n",
    "\n",
    "\n",
    "print(geocoded_work_coords.shape)\n",
    "geocoded_work_coords = gpd.sjoin(\n",
    "    geocoded_work_coords, shapes_df_all, how=\"left\", op=\"intersects\"\n",
    ")\n",
    "print(geocoded_work_coords.shape)\n",
    "geocoded_work_coords.head(3)\n",
    "\n",
    "\n",
    "geocoded_work_coords.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_work\",\n",
    "        \"iso3\": \"country_work\",\n",
    "        \"class_code\": \"class_code_work\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "geocoded_work_coords = geocoded_work_coords[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"latitude_work\",\n",
    "        \"longitude_work\",\n",
    "        \"fuaname_en_work\",\n",
    "        \"country_work\",\n",
    "        \"class_code_work\",\n",
    "    ]\n",
    "]\n",
    "geocoded_work_coords.head(3)\n",
    "geocoded_work_coords[\"work_class\"] = geocoded_work_coords.apply(\n",
    "    lambda x: \"-\" if pd.isnull(x[\"fuaname_en_work\"]) else \"sub-urban\", axis=1\n",
    ")\n",
    "geocoded_work_coords.head()\n",
    "\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "\n",
    "shapes_df_all = read_shapes(\"_core.shp\", quiet=True)\n",
    "geom = geocoded_work_coords.apply(\n",
    "    lambda x: Point([x[\"longitude_work\"], x[\"latitude_work\"]]), axis=1\n",
    ")\n",
    "geocoded_work_coords = gpd.GeoDataFrame(geocoded_work_coords, geometry=geom)\n",
    "geocoded_work_coords.crs = {\"init\": \"epsg:4326\"}\n",
    "geocoded_work_coords.head()\n",
    "\n",
    "print(geocoded_work_coords.shape)\n",
    "geocoded_work_coords = gpd.sjoin(\n",
    "    geocoded_work_coords, shapes_df_all, how=\"left\", op=\"intersects\"\n",
    ")\n",
    "print(geocoded_work_coords.shape)\n",
    "geocoded_work_coords.head(3)\n",
    "\n",
    "geocoded_work_coords.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_work2\",\n",
    "        \"iso3\": \"country_work2\",\n",
    "        \"class_code\": \"class_code_work2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "geocoded_work_coords = geocoded_work_coords[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"latitude_work\",\n",
    "        \"longitude_work\",\n",
    "        \"fuaname_en_work\",\n",
    "        \"country_work\",\n",
    "        \"fuaname_en_work2\",\n",
    "        \"country_work2\",\n",
    "        \"class_code_work2\",\n",
    "        \"work_class\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "geocoded_work_coords[\"work_class\"] = geocoded_work_coords.apply(\n",
    "    lambda x: x[\"work_class\"] if pd.isnull(x[\"fuaname_en_work2\"]) else \"urban\", axis=1\n",
    ")\n",
    "geocoded_work_coords[\"work_class\"] = geocoded_work_coords[\"work_class\"].replace(\n",
    "    \"-\", \"rural\"\n",
    ")\n",
    "\n",
    "geocoded_work_coords[\"country_work\"] = geocoded_work_coords.apply(\n",
    "    lambda x: x[\"country_work\"]\n",
    "    if pd.isnull(x[\"country_work2\"])\n",
    "    else x[\"country_work2\"],\n",
    "    axis=1,\n",
    ")\n",
    "geocoded_work_coords[\"fuaname_en_work\"] = geocoded_work_coords.apply(\n",
    "    lambda x: x[\"fuaname_en_work\"]\n",
    "    if pd.isnull(x[\"fuaname_en_work2\"])\n",
    "    else x[\"fuaname_en_work2\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "geocoded_work_coords.head(3)\n",
    "\n",
    "\n",
    "# geocoded_work_coords['work_class'] = geocoded_work_coords.apply(lambda x: x['work_class'] if pd.isnull(x['fuaname_en_work']) else classe , axis=1)\n",
    "# geocoded_work_coords.head()\n",
    "# return geocoded_work_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_work_coords[\"work_class\"] = geocoded_work_coords.apply(\n",
    "    lambda x: check_value_coords(\n",
    "        x[\"work_class\"], x[\"longitude_work\"], x[\"latitude_work\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "geocoded_work_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_work_coords.loc[\n",
    "    (geocoded_work_coords[\"latitude_work\"].isna())\n",
    "    & (geocoded_work_coords[\"longitude_work\"].isna())\n",
    "].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geocoded_work_coords.groupby(\"work_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords = geocoded_addresses[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"longitude_home\",\n",
    "        \"latitude_home\",\n",
    "    ]\n",
    "].copy()\n",
    "geocoded_home_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "shapes_df_all = read_shapes(\".shp\", quiet=True)\n",
    "\n",
    "# def classify_end_point(geocoded_home_coords, classe, level):\n",
    "geom = geocoded_home_coords.apply(\n",
    "    lambda x: Point([x[\"longitude_home\"], x[\"latitude_home\"]]), axis=1\n",
    ")\n",
    "geocoded_home_coords = gpd.GeoDataFrame(geocoded_home_coords, geometry=geom)\n",
    "geocoded_home_coords.crs = {\"init\": \"epsg:4326\"}\n",
    "geocoded_home_coords.head()\n",
    "\n",
    "\n",
    "print(geocoded_home_coords.shape)\n",
    "geocoded_home_coords = gpd.sjoin(\n",
    "    geocoded_home_coords, shapes_df_all, how=\"left\", op=\"intersects\"\n",
    ")\n",
    "print(geocoded_home_coords.shape)\n",
    "geocoded_home_coords.head(3)\n",
    "\n",
    "\n",
    "geocoded_home_coords.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_home\",\n",
    "        \"iso3\": \"country_home\",\n",
    "        \"class_code\": \"class_code_home\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "geocoded_home_coords = geocoded_home_coords[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"latitude_home\",\n",
    "        \"longitude_home\",\n",
    "        \"fuaname_en_home\",\n",
    "        \"country_home\",\n",
    "        \"class_code_home\",\n",
    "    ]\n",
    "]\n",
    "geocoded_home_coords.head(3)\n",
    "geocoded_home_coords[\"home_class\"] = geocoded_home_coords.apply(\n",
    "    lambda x: \"-\" if pd.isnull(x[\"fuaname_en_home\"]) else \"sub-urban\", axis=1\n",
    ")\n",
    "geocoded_home_coords.head()\n",
    "\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "# ======= ======= ======= ======= ======= =======\n",
    "\n",
    "shapes_df_all = read_shapes(\"_core.shp\", quiet=True)\n",
    "geom = geocoded_home_coords.apply(\n",
    "    lambda x: Point([x[\"longitude_home\"], x[\"latitude_home\"]]), axis=1\n",
    ")\n",
    "geocoded_home_coords = gpd.GeoDataFrame(geocoded_home_coords, geometry=geom)\n",
    "geocoded_home_coords.crs = {\"init\": \"epsg:4326\"}\n",
    "geocoded_home_coords.head()\n",
    "\n",
    "print(geocoded_home_coords.shape)\n",
    "geocoded_home_coords = gpd.sjoin(\n",
    "    geocoded_home_coords, shapes_df_all, how=\"left\", op=\"intersects\"\n",
    ")\n",
    "print(geocoded_home_coords.shape)\n",
    "geocoded_home_coords.head(3)\n",
    "\n",
    "geocoded_home_coords.rename(\n",
    "    columns={\n",
    "        \"fuaname_en\": \"fuaname_en_home2\",\n",
    "        \"iso3\": \"country_home2\",\n",
    "        \"class_code\": \"class_code_home2\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "geocoded_home_coords = geocoded_home_coords[\n",
    "    [\n",
    "        \"userid\",\n",
    "        \"latitude_home\",\n",
    "        \"longitude_home\",\n",
    "        \"fuaname_en_home\",\n",
    "        \"country_home\",\n",
    "        \"fuaname_en_home2\",\n",
    "        \"country_home2\",\n",
    "        \"class_code_home2\",\n",
    "        \"home_class\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "geocoded_home_coords[\"home_class\"] = geocoded_home_coords.apply(\n",
    "    lambda x: x[\"home_class\"] if pd.isnull(x[\"fuaname_en_home2\"]) else \"urban\", axis=1\n",
    ")\n",
    "geocoded_home_coords[\"home_class\"] = geocoded_home_coords[\"home_class\"].replace(\n",
    "    \"-\", \"rural\"\n",
    ")\n",
    "\n",
    "geocoded_home_coords[\"country_home\"] = geocoded_home_coords.apply(\n",
    "    lambda x: x[\"country_home\"]\n",
    "    if pd.isnull(x[\"country_home2\"])\n",
    "    else x[\"country_home2\"],\n",
    "    axis=1,\n",
    ")\n",
    "geocoded_home_coords[\"fuaname_en_home\"] = geocoded_home_coords.apply(\n",
    "    lambda x: x[\"fuaname_en_home\"]\n",
    "    if pd.isnull(x[\"fuaname_en_home2\"])\n",
    "    else x[\"fuaname_en_home2\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# geocoded_home_coords['home_class'] = geocoded_home_coords.apply(lambda x: x['home_class'] if pd.isnull(x['fuaname_en_home']) else classe , axis=1)\n",
    "# geocoded_home_coords.head()\n",
    "# return geocoded_home_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords[\"home_class\"] = geocoded_home_coords.apply(\n",
    "    lambda x: check_value_coords(\n",
    "        x[\"home_class\"], x[\"longitude_home\"], x[\"latitude_home\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "geocoded_home_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords.loc[\n",
    "    (geocoded_home_coords[\"latitude_home\"].isna())\n",
    "    & (geocoded_home_coords[\"longitude_home\"].isna())\n",
    "].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords.groupby(\"home_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_work_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_home_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_user_home_work_coords = pd.merge(\n",
    "    geocoded_work_coords[[\"userid\", \"latitude_work\", \"longitude_work\", \"work_class\"]],\n",
    "    geocoded_home_coords[[\"userid\", \"latitude_home\", \"longitude_home\", \"home_class\"]],\n",
    "    on=\"userid\",\n",
    ")\n",
    "print(geocoded_user_home_work_coords.shape)\n",
    "geocoded_user_home_work_coords.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoded_user_home_work_coords.to_pickle(\n",
    "    input_path + \"geocoded_user_home_work_coords.pkl\"\n",
    ")\n",
    "print(input_path + \"geocoded_user_home_work_coords.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_user_coords = geocoded_user_coords.copy()\n",
    "\n",
    "anon_user_coords[\"latitude_work\"] = anon_user_coords.apply(\n",
    "    lambda row: anonymize_coord_nan(row[\"latitude_work\"], row[\"work_class\"]), axis=1\n",
    ")\n",
    "anon_user_coords[\"longitude_work\"] = anon_user_coords.apply(\n",
    "    lambda row: anonymize_coord_nan(row[\"longitude_work\"], row[\"work_class\"]), axis=1\n",
    ")\n",
    "\n",
    "anon_user_coords[\"latitude_home\"] = anon_user_coords.apply(\n",
    "    lambda row: anonymize_coord_nan(row[\"latitude_home\"], row[\"home_class\"]), axis=1\n",
    ")\n",
    "anon_user_coords[\"longitude_home\"] = anon_user_coords.apply(\n",
    "    lambda row: anonymize_coord_nan(row[\"longitude_home\"], row[\"home_class\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_user_coords.fillna(\"\", inplace=True)\n",
    "\n",
    "anon_user_coords.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"users_coordinates.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "print(\"output_path: \", output_path)\n",
    "\n",
    "anon_user_coords.to_csv(output_path, index=False, header=True, float_format=\"%.3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
