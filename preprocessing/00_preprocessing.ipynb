{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Steps\n",
    "\n",
    "### - [user_setting_mapping](#user_setting_mapping)\n",
    "\n",
    "### - [trips_users_df](#trips_users_df)\n",
    "\n",
    "1. Read all data until 30-10-2019\n",
    "2. Read updated data until 16-12-2019\n",
    "3. Merge and create `trips_users_df`\n",
    "\n",
    "### - [users_df_with_trips](#users_df_with_trips)\n",
    "\n",
    "1. Read `usersData_normalized` for each campain that contains data about **all** users (updated to 16-12-2019)\n",
    "2. Remove users without trips\n",
    "3. Add home_address and work_address\n",
    "4. Preprocess home and work address **TODO**\n",
    "5. Extract generic worthwhileness values\n",
    "6. Extract specific worthwhileness values\n",
    "7. Solve language problem\n",
    "8. Save `users_df_with_trips`\n",
    "\n",
    "### - [trips_df](#trips_df)\n",
    "\n",
    "1. Read all data until 30-10-2019\n",
    "2. Read updated data until 16-12-2019\n",
    "3. Merge and create `trips_df`\n",
    "4. Take only trips also in `trips_users_df`\n",
    "5. Preprocessing on dates\n",
    "6. Save\n",
    "7. Extract objectives\n",
    "\n",
    "### - [all_legs](#all_legs)\n",
    "\n",
    "1. Extract legs from `trips_df`\n",
    "2. Preprocessing on legs\n",
    "3. Add category transport\n",
    "4. Add leg duration and preprocessing on duration and distance\n",
    "\n",
    "\n",
    "### - [all_legs_final_ds_user_info](#all_legs_final_ds_user_info)\n",
    "\n",
    "1. Merge legs info with users info and trip info\n",
    "\n",
    "### - [all_legs_merged](#all_legs_merged)\n",
    "\n",
    "1. Merge legs\n",
    "2. Average Wasted Time for merged legs\n",
    "\n",
    "### - [all_factors](#all_factors)\n",
    "\n",
    "1. Select factors from each leg\n",
    "2. Select generic activities from each leg\n",
    "3. Create the Encodings\n",
    "4. Map the encoded factors\n",
    "5. Check not encoded factors\n",
    "\n",
    "### - [values_from_trip](#values_from_trip)\n",
    "\n",
    "1. Extract values from trips\n",
    "\n",
    "### - [outliers_detection](#outliers_detection)\n",
    "\n",
    "\n",
    "## Savings\n",
    "\n",
    "- `trips_users_df.pkl`\n",
    "- `all_users_df.pkl`\n",
    "- `gen_worthwhile.pkl`\n",
    "- `specific_worthwhile.pkl`\n",
    "- `mot_text.txt`\n",
    "- `mot_motText.txt`\n",
    "- `users_df_with_trips.pkl`\n",
    "- `trips_df_original.pkl`\n",
    "- `trip_objs.pkl`\n",
    "- `original_all_legs.pkl`\n",
    "- `all_legs.pkl`\n",
    "- `category_transp_mode_dict.json`\n",
    "- `all_legs_final_ds_user_info.pkl`\n",
    "- `all_legs_merged.pkl`\n",
    "- `merged_legs.json`\n",
    "- `all_legs_merged_1.pkl`\n",
    "- `all_factors_original.pkl`\n",
    "- `all_generic_activities.pkl`\n",
    "- `all_factors.pkl`\n",
    "- `values_from_trip.pkl`\n",
    "- `all_legs_merged_no_outlier_0.01.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style\"))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "CUTTING_DATE = \"2019-05-01\"  # remove trips and data published before this date\n",
    "\n",
    "meta_data_path = \"../../data-campaigns/meta-data/\"\n",
    "\n",
    "# ========= ========= ========= ========= =========\n",
    "input_path = \"../../data-campaigns/2019-10-30.all/\"\n",
    "update_path = \"../../data-campaigns/2019-12-16.update/\"\n",
    "out_path = \"../../2019-12-16.out/\"\n",
    "\n",
    "# anon dataset (output)\n",
    "anon_dataset_dir = out_path.rstrip(\"/\").split(\"/\")[-1].replace(\".out\", \".anon\")\n",
    "anon_dataset_path = os.path.join(\"../..\", \"anon-dataset\", anon_dataset_dir)\n",
    "# ========= ========= ========= ========= ========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(os.path.abspath(out_path))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(out_path), file=sys.stderr)\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.abspath(anon_dataset_path))\n",
    "except FileExistsError:\n",
    "    print(\"Directory '{}' already exists\".format(anon_dataset_path), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='user_setting_mapping'></a>\n",
    "### user_setting_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_setting_mapping = pd.read_csv(\n",
    "    meta_data_path + \"user-settings-values-mapping-unfolded_original.csv\",\n",
    "    encoding=\"cp1252\",\n",
    "    header=None,\n",
    ")\n",
    "user_setting_mapping.columns = [\"dbvalues\", \"value\"]\n",
    "user_setting_mapping = user_setting_mapping.dropna()\n",
    "\n",
    "user_setting_mapping_dict = {}\n",
    "for ix, row in user_setting_mapping.iterrows():\n",
    "    user_setting_mapping_dict[row[\"dbvalues\"]] = row[\"value\"]\n",
    "\n",
    "# user_setting_mapping_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trips_users_df'></a>\n",
    "### trips_users_df\n",
    "\n",
    "- Read all data until 30-10-2019\n",
    "- Read updated data until 16-12-2019\n",
    "- Merge and create `trips_users_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. All data until 30-10-2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. All data\n",
    "\n",
    "trips_prev_users_df = None\n",
    "tempdf = None\n",
    "\n",
    "read_files = glob.glob(input_path + \"*_tripsUsersRelationData.json\")\n",
    "print(read_files)\n",
    "\n",
    "count_trips = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips_users = json.loads(f.read())\n",
    "        tempdf = json_normalize(trips_users)\n",
    "\n",
    "    print(tempdf.shape)\n",
    "    count_trips += tempdf.shape[0]\n",
    "\n",
    "    if trips_prev_users_df is None:\n",
    "        trips_prev_users_df = tempdf\n",
    "    else:\n",
    "        trips_prev_users_df = pd.concat([trips_prev_users_df, tempdf])\n",
    "\n",
    "print(\"Trips = \", count_trips)\n",
    "print(trips_prev_users_df.shape)\n",
    "\n",
    "print(\"Remove duplicates...\")\n",
    "trips_prev_users_df = trips_prev_users_df.drop_duplicates(keep=\"first\")\n",
    "print(trips_prev_users_df.shape)\n",
    "trips_prev_users_df[\"startDate_formated\"] = pd.to_datetime(\n",
    "    trips_prev_users_df[\"startDate\"], unit=\"ms\"\n",
    ")\n",
    "trips_prev_users_df = trips_prev_users_df[\n",
    "    trips_prev_users_df[\"startDate_formated\"] >= CUTTING_DATE\n",
    "]\n",
    "print(\"total trips: \", trips_prev_users_df.shape[0])\n",
    "print(\"total unique trips: \", len(trips_prev_users_df[\"tripid\"].unique()))\n",
    "trips_prev_users_df[\"startDateDay\"] = trips_prev_users_df[\"startDate_formated\"].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Updated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Updated data\n",
    "\n",
    "temp = None\n",
    "trips_update_users_df = None\n",
    "\n",
    "read_files = glob.glob(update_path + \"*_tripsUsersRelationData.json\")\n",
    "print(read_files)\n",
    "\n",
    "count_trips_update = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips_users = json.loads(f.read())\n",
    "        temp = json_normalize(trips_users)\n",
    "\n",
    "    print(temp.shape)\n",
    "    count_trips_update += temp.shape[0]\n",
    "\n",
    "    if trips_update_users_df is None:\n",
    "        trips_update_users_df = temp\n",
    "    else:\n",
    "        trips_update_users_df = pd.concat([trips_update_users_df, temp])\n",
    "\n",
    "print(\"Trips (update) = \", count_trips_update)\n",
    "print(trips_update_users_df.shape)\n",
    "\n",
    "print(\"Remove duplicates...\")\n",
    "trips_update_users_df = trips_update_users_df.drop_duplicates(keep=\"first\")\n",
    "print(trips_update_users_df.shape)\n",
    "trips_update_users_df[\"startDate_formated\"] = pd.to_datetime(\n",
    "    trips_update_users_df[\"startDate\"], unit=\"ms\"\n",
    ")\n",
    "trips_update_users_df = trips_update_users_df[\n",
    "    trips_update_users_df[\"startDate_formated\"] >= CUTTING_DATE\n",
    "]\n",
    "print(\"total trips: \", trips_update_users_df.shape[0])\n",
    "print(\"total unique trips: \", len(trips_update_users_df[\"tripid\"].unique()))\n",
    "trips_update_users_df[\"startDateDay\"] = trips_update_users_df[\n",
    "    \"startDate_formated\"\n",
    "].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "print(\"trips DF shape: {}\".format(trips_prev_users_df.shape))\n",
    "print(\"trips_update DF shape: {}\".format(trips_update_users_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Merge all + updated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Merge all + updated\n",
    "trips_users_df = pd.concat([trips_prev_users_df, trips_update_users_df])\n",
    "print(\"trips DF shape: {}\".format(trips_users_df.shape))\n",
    "\n",
    "print(\"Remove duplicates...\")\n",
    "trips_users_df = trips_users_df.drop_duplicates(keep=\"first\")\n",
    "print(\"trips DF shape: {}\".format(trips_users_df.shape))\n",
    "print()\n",
    "print(\"Total trips: \", trips_users_df.shape[0])\n",
    "print(\"Total unique trips: \", len(trips_users_df[\"tripid\"].unique()))\n",
    "print(\"Total number of unique users: \", len(trips_users_df[\"userid\"].unique()))\n",
    "\n",
    "## SAVE\n",
    "trips_users_df.to_pickle(out_path + \"trips_users_df.pkl\")\n",
    "trips_users_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='users_df_with_trips'></a>\n",
    "### users_df_with_trips\n",
    "\n",
    "1. Read `usersData_normalized` for each campain that contains data about **all** users (updated to 16-12-2019)\n",
    "2. Remove users without trips\n",
    "3. Add home_address and work_address\n",
    "4. Preprocess home and work address **TODO**\n",
    "5. Extract generic worthwhileness values\n",
    "6. Extract specific worthwhileness values\n",
    "7. Solve language problem\n",
    "8. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Read *all* users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Read *all* users\n",
    "users_df = []\n",
    "first = True\n",
    "read_files = glob.glob(input_path + \"*_usersData_normalized.json\")\n",
    "print(read_files)\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        users = json.loads(f.read())\n",
    "        users_df_temp = json_normalize(users)\n",
    "    print(users_df_temp.shape)\n",
    "    tot += users_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        users_df = users_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        users_df = pd.concat([users_df, users_df_temp])\n",
    "\n",
    "print()\n",
    "print(\"Tot = \", tot)\n",
    "print(users_df.shape)\n",
    "\n",
    "print()\n",
    "print(\"Remove duplicates...\")\n",
    "users_df = users_df.drop_duplicates([\"userid\"], keep=\"first\")\n",
    "print(users_df.shape)\n",
    "\n",
    "### SAVE all users\n",
    "users_df.to_pickle(out_path + \"all_users_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Remove users without trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Remove users without trips\n",
    "print()\n",
    "print(\n",
    "    \"Remove users that do not perform at least 1 trip i.e. remove users without trips\"\n",
    ")\n",
    "users_df_with_trips = users_df[\n",
    "    users_df.userid.isin(list(trips_users_df.userid.unique()))\n",
    "]\n",
    "### Rename columns\n",
    "users_df_with_trips.columns = [\n",
    "    w.replace(\"userSettings.\", \"\") for w in users_df_with_trips.columns\n",
    "]\n",
    "\n",
    "print(\"- Unique users:\", users_df[\"userid\"].unique().shape[0])\n",
    "print(\"- Users with at least 1 trip:\", users_df_with_trips.shape[0])\n",
    "\n",
    "users_with_notrips = users_df[\n",
    "    ~users_df.userid.isin(list(trips_users_df.userid.unique()))\n",
    "]\n",
    "print(\"- Users with no trips:\", users_with_notrips.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Add homeAddress and workAddress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "users_df_address = pd.DataFrame()\n",
    "first = True\n",
    "file_path = meta_data_path + \"user-data-w-addresses/\"\n",
    "read_files = glob.glob(file_path + \"*_Global_usersData.json\")\n",
    "print(read_files)\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        users = json.loads(f.read())\n",
    "        users_df_temp = json_normalize(users)\n",
    "    print(users_df_temp.shape)\n",
    "    tot += users_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        users_df_address = users_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        users_df_address = pd.concat([users_df_address, users_df_temp])\n",
    "\n",
    "print()\n",
    "print(users_df_address.shape)\n",
    "print(\"Remove duplicates...\")\n",
    "users_df_address = users_df_address.drop_duplicates([\"userid\"], keep=\"first\")\n",
    "print(users_df_address.shape)\n",
    "print(\"unique users:\", len(users_df_address.userid.unique()))\n",
    "\n",
    "### Rename columns\n",
    "users_df_address.columns = [\n",
    "    w.replace(\"userSettings.\", \"\") for w in users_df_address.columns\n",
    "]\n",
    "\n",
    "### Add the address to users_df_with_trips\n",
    "print()\n",
    "print(\"add home and work address to users_df_with_trips..\")\n",
    "users_df_with_trips = pd.merge(\n",
    "    users_df_with_trips,\n",
    "    users_df_address[\n",
    "        [\n",
    "            \"userid\",\n",
    "            \"workAddress._id\",\n",
    "            \"workAddress.address\",\n",
    "            \"homeAddress._id\",\n",
    "            \"homeAddress.address\",\n",
    "        ]\n",
    "    ],\n",
    "    on=\"userid\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(users_df_with_trips.shape)\n",
    "print(\n",
    "    \"users with home address\",\n",
    "    len(users_df_with_trips[~users_df_with_trips[\"homeAddress.address\"].isna()]),\n",
    ")\n",
    "print(\n",
    "    \"users with work address\",\n",
    "    len(users_df_with_trips[~users_df_with_trips[\"workAddress.address\"].isna()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. preprocess home and work**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Extract generic worthwhileness values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_worthwhile = users_df_with_trips[[\"userid\", \"actValue\", \"prodValue\", \"relValue\"]]\n",
    "gen_worthwhile.columns = [\"userid\", \"fitness\", \"productivity\", \"enjoyment\"]\n",
    "\n",
    "## Save\n",
    "gen_worthwhile.to_pickle(out_path + \"gen_worthwhile.pkl\")\n",
    "\n",
    "print(\"users with generic WW:\", len(gen_worthwhile.userid.unique()))\n",
    "gen_worthwhile.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. (b) Extract generic worthwhileness values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_generic_worthwhileness_table = gen_worthwhile.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstorename = {\n",
    "    \"userid\": \"userid\",\n",
    "    \"fitness\": \"fit\",\n",
    "    \"productivity\": \"prod\",\n",
    "    \"enjoyment\": \"enjoy\",\n",
    "}\n",
    "\n",
    "columnstokeep = [col for col in colstorename.keys()]\n",
    "csv_columns = [col for col in colstorename.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_generic_worthwhileness_table.rename(columns=colstorename, inplace=True)\n",
    "print(user_generic_worthwhileness_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstypes = {\n",
    "    \"userid\": \"str\",\n",
    "    \"fit\": \"int\",\n",
    "    \"prod\": \"int\",\n",
    "    \"enjoy\": \"int\",\n",
    "}\n",
    "\n",
    "user_generic_worthwhileness_table = user_generic_worthwhileness_table.astype(colstypes)\n",
    "user_generic_worthwhileness_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to csv\n",
    "output_file = \"user_generic_worthwhileness_values.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "\n",
    "user_generic_worthwhileness_table.to_csv(\n",
    "    output_path, index=False, header=True, columns=csv_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Extract specific worthwhileness values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_worthwhile = []\n",
    "first = True\n",
    "for index, user in users_df_with_trips.iterrows():\n",
    "\n",
    "    specific_worthwhile = pd.DataFrame(list(user[\"preferedMots\"]))\n",
    "    specific_worthwhile[\"userid\"] = user[\"userid\"]\n",
    "    if first:\n",
    "        all_users_pref_mot = specific_worthwhile\n",
    "        first = False\n",
    "    else:\n",
    "        all_users_pref_mot = pd.concat(\n",
    "            [all_users_pref_mot, specific_worthwhile], sort=True\n",
    "        )\n",
    "\n",
    "specific_worthwhile.columns = [\n",
    "    \"Mot\",\n",
    "    \"MotText\",\n",
    "    \"_id\",\n",
    "    \"fitness\",\n",
    "    \"productivity\",\n",
    "    \"enjoyment\",\n",
    "    \"userid\",\n",
    "]\n",
    "\n",
    "## Save\n",
    "all_users_pref_mot.to_pickle(out_path + \"specific_worthwhile.pkl\")\n",
    "\n",
    "print(\"users with specific WW:\", len(all_users_pref_mot.userid.unique()))\n",
    "all_users_pref_mot.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save different motText and association mot - motText\n",
    "\n",
    "# mot_text\n",
    "mottext_lst = list(all_users_pref_mot.MotText.unique())\n",
    "with open(out_path + \"mot_text.txt\", \"w\") as f:\n",
    "    for item in mottext_lst:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# mot - motText\n",
    "motcode_dict = {}\n",
    "for motcode in list(all_users_pref_mot.Mot.unique()):\n",
    "    motcode_dict[motcode] = list(\n",
    "        all_users_pref_mot[\"MotText\"][all_users_pref_mot.Mot == motcode].unique()\n",
    "    )\n",
    "\n",
    "with open(out_path + \"mot_motText.txt\", \"w\") as f:\n",
    "    for k, v in motcode_dict.items():\n",
    "        f.write(\"'%s':'%s', \\n\" % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. (b) Extract specific worthwhileness values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specific_worthwhileness_table = all_users_pref_mot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specific_worthwhileness_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstorename = {\n",
    "    \"userid\": \"userid\",\n",
    "    \"Mot\": \"motid\",\n",
    "    \"motsFit\": \"fit\",\n",
    "    \"motsProd\": \"prod\",\n",
    "    \"motsRelax\": \"enjoy\",\n",
    "}\n",
    "\n",
    "columnstokeep = [col for col in colstorename.keys()]\n",
    "colstodrop = set(user_specific_worthwhileness_table.columns) - set(columnstokeep)\n",
    "csv_columns = [col for col in colstorename.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specific_worthwhileness_table.drop(colstodrop, axis=1, inplace=True)\n",
    "print(user_specific_worthwhileness_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specific_worthwhileness_table.rename(columns=colstorename, inplace=True)\n",
    "print(user_specific_worthwhileness_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_specific_worthwhileness_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstypes = {\n",
    "    \"userid\": \"str\",\n",
    "    \"motid\": \"int\",\n",
    "    \"fit\": \"int\",\n",
    "    \"prod\": \"int\",\n",
    "    \"enjoy\": \"int\",\n",
    "}\n",
    "\n",
    "user_specific_worthwhileness_table = user_specific_worthwhileness_table.astype(\n",
    "    colstypes\n",
    ")\n",
    "user_specific_worthwhileness_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to csv\n",
    "output_file = \"user_specific_worthwhileness_values.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "\n",
    "user_specific_worthwhileness_table.to_csv(\n",
    "    output_path, index=False, header=True, columns=csv_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Solve language problem and encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# gender\n",
    "print(users_df_with_trips[\"gender\"].unique())\n",
    "print(\"\\n MAKE MAPPING FROM OTHER LANGUAGES TO EN\\n\")\n",
    "\n",
    "users_df_with_trips[\"gender_en\"] = users_df_with_trips[\"gender\"].apply(\n",
    "    lambda x: user_setting_mapping_dict.get(x)\n",
    ")\n",
    "print(users_df_with_trips[\"gender_en\"].unique())\n",
    "\n",
    "# Language\n",
    "users_df_with_trips[\"lang\"] = users_df_with_trips[\"lang\"].apply(\n",
    "    lambda x: \"eng\" if isinstance(x, str) and x.lower() == \"english\" else x\n",
    ")\n",
    "users_df_with_trips[\"lang\"] = users_df_with_trips[\"lang\"].fillna(\"-\")\n",
    "\n",
    "# Education\n",
    "print(users_df_with_trips[\"degree\"].unique())\n",
    "users_df_with_trips[\"degree_en\"] = users_df_with_trips[\"degree\"].apply(\n",
    "    lambda x: user_setting_mapping_dict.get(x)\n",
    ")\n",
    "print(\"Make mapping...\")\n",
    "print(users_df_with_trips[\"degree_en\"].unique())\n",
    "# Fill missing values\n",
    "users_df_with_trips[\"degree_en\"] = users_df_with_trips[\"degree_en\"].fillna(\"-\")\n",
    "\n",
    "\n",
    "# maritalStatus\n",
    "# Fill missings\n",
    "users_df_with_trips[[\"maritalStatusHousehold\"]] = users_df_with_trips[\n",
    "    [\"maritalStatusHousehold\"]\n",
    "].fillna(value=\"-\")\n",
    "users_df_with_trips[\"maritalStatusHousehold\"] = users_df_with_trips[\n",
    "    \"maritalStatusHousehold\"\n",
    "].apply(lambda x: \"-\" if x == \"\" or x == \" \" else x)\n",
    "\n",
    "print(users_df_with_trips[\"maritalStatusHousehold\"].unique())\n",
    "print(\"Make mapping...\")\n",
    "users_df_with_trips[\"maritalStatus\"] = users_df_with_trips[\n",
    "    \"maritalStatusHousehold\"\n",
    "].apply(lambda x: user_setting_mapping_dict.get(x))\n",
    "print(users_df_with_trips[\"maritalStatus\"].unique())\n",
    "users_df_with_trips[[\"maritalStatus\"]] = users_df_with_trips[[\"maritalStatus\"]].fillna(\n",
    "    value=\"-\"\n",
    ")\n",
    "\n",
    "\n",
    "# Number of people in household\n",
    "users_df_with_trips[[\"numberPeopleHousehold\"]] = users_df_with_trips[\n",
    "    [\"numberPeopleHousehold\"]\n",
    "].fillna(value=\"-\")\n",
    "users_df_with_trips[\"numberPeopleHousehold\"] = users_df_with_trips[\n",
    "    \"numberPeopleHousehold\"\n",
    "].apply(lambda x: \"-\" if x == \"\" or x == \" \" else x)\n",
    "\n",
    "# years of residence in country\n",
    "users_df_with_trips[[\"yearsOfResidenceHousehold\"]] = users_df_with_trips[\n",
    "    [\"yearsOfResidenceHousehold\"]\n",
    "].fillna(value=\"-\")\n",
    "users_df_with_trips[\"yearsOfResidenceHousehold\"] = users_df_with_trips[\n",
    "    \"yearsOfResidenceHousehold\"\n",
    "].apply(lambda x: \"-\" if x == \"\" or x == \" \" else x)\n",
    "\n",
    "# Labour status\n",
    "# Remove missing\n",
    "users_df_with_trips[[\"labourStatusHousehold\"]] = users_df_with_trips[\n",
    "    [\"labourStatusHousehold\"]\n",
    "].fillna(value=\"-\")\n",
    "users_df_with_trips[[\"labourStatusHousehold\"]] = users_df_with_trips[\n",
    "    \"labourStatusHousehold\"\n",
    "].apply(lambda x: \"-\" if x == \"\" or x == \" \" else x)\n",
    "\n",
    "users_df_with_trips[\"labourStatusHousehold_en\"] = users_df_with_trips[\n",
    "    \"labourStatusHousehold\"\n",
    "].apply(lambda x: user_setting_mapping_dict.get(x))\n",
    "print(users_df_with_trips[\"labourStatusHousehold\"].unique())\n",
    "print(\"Make mapping...\")\n",
    "print(users_df_with_trips[\"labourStatusHousehold_en\"].unique())\n",
    "users_df_with_trips[[\"labourStatusHousehold_en\"]] = users_df_with_trips[\n",
    "    [\"labourStatusHousehold_en\"]\n",
    "].fillna(value=\"-\")\n",
    "\n",
    "\n",
    "## Age\n",
    "users_df_with_trips[\"age\"] = users_df_with_trips.apply(\n",
    "    lambda x: str(x[\"minAge\"]) + \"-\" + str(x[\"maxAge\"]), axis=1\n",
    ")\n",
    "## 16-24\n",
    "users_df_with_trips[\"age\"] = users_df_with_trips[\"age\"].apply(\n",
    "    lambda x: \"16-24\" if x.startswith((\"16-\", \"20-\")) else x\n",
    ")\n",
    "\n",
    "## 25-49\n",
    "users_df_with_trips[\"age\"] = users_df_with_trips[\"age\"].apply(\n",
    "    lambda x: \"25-49\" if x.startswith((\"25-\", \"30-\", \"36-\", \"40-\")) else x\n",
    ")\n",
    "\n",
    "## 50-64\n",
    "users_df_with_trips[\"age\"] = users_df_with_trips[\"age\"].apply(\n",
    "    lambda x: \"50-64\" if x.startswith(\"50-\") else x\n",
    ")\n",
    "\n",
    "## 65+\n",
    "users_df_with_trips[\"age\"] = users_df_with_trips[\"age\"].apply(\n",
    "    lambda x: \"65+\" if x.startswith((\"65-\", \"75-\")) else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Campaigns\n",
    "\n",
    "campaigns_path = os.path.join(meta_data_path, \"campaignsInfo.csv\")\n",
    "campaigns = pd.read_csv(campaigns_path)\n",
    "# create a mapping campaign_id - country\n",
    "campaignid_country_dict = pd.Series(\n",
    "    campaigns[\"country\"].values, index=campaigns[\"campaignId\"]\n",
    ").to_dict()\n",
    "\n",
    "#################\n",
    "# users_df_with_trips = pd.read_pickle(input_path + 'users_df_with_trips.pkl')\n",
    "\n",
    "\n",
    "# there are cases with '' in the list -> ex. ['5cbddfecb7d8af05386ec66e', '']\n",
    "# there is a user with 3 campaign ids not encoded.\n",
    "# they are '5b8fb4004dd33478cd30a8b1', '5b27f92c49ee1d1742cacc75', '5b3b4e5cb3863f55591ec50d'\n",
    "# and appear only once\n",
    "wrong_codes = [\n",
    "    \"\",\n",
    "    \"5b8fb4004dd33478cd30a8b1\",\n",
    "    \"5b27f92c49ee1d1742cacc75\",\n",
    "    \"5b3b4e5cb3863f55591ec50d\",\n",
    "]\n",
    "\n",
    "user_with_1camp = 0\n",
    "user_with_more_camp = 0\n",
    "user_with_more_camp_same_country = 0\n",
    "\n",
    "users_df_with_trips[\"onCampaigns_cor\"] = users_df_with_trips[\"onCampaigns\"]\n",
    "for idx, row in users_df_with_trips.iterrows():\n",
    "    lst = row[\"onCampaigns\"]\n",
    "\n",
    "    # user with single campaign\n",
    "    if len(lst) == 1:\n",
    "        users_df_with_trips.loc[idx, \"onCampaigns_cor\"] = campaignid_country_dict[\n",
    "            lst[0]\n",
    "        ]\n",
    "        user_with_1camp += 1\n",
    "\n",
    "    # user with more campaigns\n",
    "    if len(lst) > 1:\n",
    "\n",
    "        new_lst = [\n",
    "            campaignid_country_dict[lst[i]]\n",
    "            for i in range(len(lst))\n",
    "            if lst[i] not in wrong_codes\n",
    "        ]\n",
    "\n",
    "        # select unique countries\n",
    "        new_lst_nodupl = list(dict.fromkeys(new_lst))\n",
    "\n",
    "        # users with more campaigns but same country\n",
    "        if len(new_lst_nodupl) == 1:\n",
    "            user_with_more_camp_same_country += 1\n",
    "            users_df_with_trips.loc[idx, \"onCampaigns_cor\"] = new_lst_nodupl[0]\n",
    "\n",
    "        # users with more campaigns and different countries\n",
    "        else:\n",
    "            user_with_more_camp += 1\n",
    "            # print(new_lst_nodupl)\n",
    "\n",
    "            # assign these user to 'AAA'\n",
    "            users_df_with_trips.loc[idx, \"onCampaigns_cor\"] = \"AAA\"\n",
    "\n",
    "\n",
    "print(\"tot users:\", users_df_with_trips.userid.nunique())\n",
    "print(\"users with 1 campaign: \", user_with_1camp)\n",
    "print(\"users with more campaigns but same country: \", user_with_more_camp_same_country)\n",
    "print(\"users with more campaigns and different countries: \", user_with_more_camp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 'CHE' to the class Other (AAA)\n",
    "users_df_with_trips[\"onCampaigns_cor\"] = users_df_with_trips[\"onCampaigns_cor\"].apply(\n",
    "    lambda x: \"AAA\" if x == \"CHE\" else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips[\"onCampaigns\"] = users_df_with_trips[\"onCampaigns_cor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips.drop(\"onCampaigns_cor\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips.groupby(\"onCampaigns\").size().sort_values(\n",
    "    ascending=False\n",
    ").reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df_with_trips.to_pickle(out_path + \"users_df_with_trips.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Export CSV table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table = users_df_with_trips.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstorename = {\n",
    "    \"userid\": \"userid\",\n",
    "    \"registerTimestamp\": \"registration_date\",\n",
    "    \"gender_en\": \"gender\",\n",
    "    \"age\": \"age_range\",\n",
    "    \"lang\": \"lang\",\n",
    "    \"city\": \"city\",\n",
    "    \"country\": \"country\",\n",
    "    \"degree_en\": \"education_level\",\n",
    "    \"maritalStatusHousehold\": \"marital_status_household\",\n",
    "    \"numberPeopleHousehold\": \"number_people_household\",\n",
    "    \"labourStatusHousehold_en\": \"labour_status_household\",\n",
    "    \"yearsOfResidenceHousehold\": \"years_of_residence_household\",\n",
    "}\n",
    "\n",
    "columnstokeep = [col for col in colstorename.keys()]\n",
    "colstodrop = set(users_details_table.columns) - set(columnstokeep)\n",
    "csv_columns = [col for col in colstorename.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table.drop(colstodrop, axis=1, inplace=True)\n",
    "print(users_details_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table.rename(columns=colstorename, inplace=True)\n",
    "print(users_details_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table[\"registration_date\"] = pd.to_datetime(\n",
    "    users_details_table[\"registration_date\"], unit=\"ms\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat = np.datetime64(\"NaT\")\n",
    "\n",
    "\n",
    "def convert_date(timestamp):\n",
    "    if not pd.isnull(timestamp):\n",
    "        return datetime.strftime(timestamp, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "users_details_table[\"registration_date\"] = users_details_table[\"registration_date\"].map(\n",
    "    convert_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_details_table = users_details_table.replace(\"-\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colstypes = {\n",
    "    \"userid\": \"str\",\n",
    "    \"registration_date\": \"str\",\n",
    "    \"gender\": \"str\",\n",
    "    \"age_range\": \"str\",\n",
    "    \"lang\": \"str\",\n",
    "    \"city\": \"str\",\n",
    "    \"country\": \"str\",\n",
    "    \"education_level\": \"str\",\n",
    "    \"marital_status_household\": \"str\",\n",
    "    \"number_people_household\": \"str\",\n",
    "    \"labour_status_household\": \"str\",\n",
    "    \"years_of_residence_household\": \"str\",\n",
    "}\n",
    "\n",
    "users_details_table.astype(colstypes)\n",
    "users_details_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_details_table.groupby(\"marital_status_household\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to csv\n",
    "output_file = \"user_details.csv\"\n",
    "output_path = os.path.join(anon_dataset_path, output_file)\n",
    "users_details_table.to_csv(output_path, index=False, header=True, columns=csv_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trips_df'></a>\n",
    "### trips_df\n",
    "\n",
    "1. Read all data until 30-10-2019\n",
    "2. Read updated data until 16-12-2019\n",
    "3. Merge and create `trips_df`\n",
    "4. Take only trips also in `trips_users_df`\n",
    "5. Preprocessing\n",
    "6. Save\n",
    "7. Extract objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. All trip data until 30-10-2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. All trips until 30-10-2019\n",
    "\n",
    "trips_prev_df = []\n",
    "first = True\n",
    "read_files = glob.glob(input_path + \"*_tripsData.json\")\n",
    "print(read_files)\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips = json.loads(f.read())\n",
    "        trips_prev_df_temp = json_normalize(trips)\n",
    "    print(trips_prev_df_temp.shape)\n",
    "    tot += trips_prev_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        trips_prev_df = trips_prev_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        trips_prev_df = pd.concat([trips_prev_df, trips_prev_df_temp])\n",
    "\n",
    "print()\n",
    "print(\"Tot = \", tot)\n",
    "print(trips_prev_df.shape)\n",
    "print()\n",
    "print(\"Remove duplicates...\")\n",
    "trips_prev_df_nodup = trips_prev_df.drop_duplicates([\"tripid\"], keep=\"first\")\n",
    "print(trips_prev_df_nodup.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Updated trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Updated trips\n",
    "\n",
    "trips_updated_df = []\n",
    "first = True\n",
    "read_files = glob.glob(update_path + \"*_tripsData.json\")\n",
    "print(read_files)\n",
    "\n",
    "tot = 0\n",
    "for f in read_files:\n",
    "    print(f)\n",
    "    with open(f) as f:\n",
    "        trips = json.loads(f.read())\n",
    "        trips_updated_df_temp = json_normalize(trips)\n",
    "    print(trips_updated_df_temp.shape)\n",
    "    tot += trips_updated_df_temp.shape[0]\n",
    "\n",
    "    if first:\n",
    "        trips_updated_df = trips_updated_df_temp\n",
    "        first = False\n",
    "    else:\n",
    "        trips_updated_df = pd.concat(\n",
    "            [trips_updated_df, trips_updated_df_temp], sort=True\n",
    "        )\n",
    "\n",
    "print()\n",
    "print(\"Tot = \", tot)\n",
    "print(trips_updated_df.shape)\n",
    "print()\n",
    "print(\"Remove duplicates...\")\n",
    "trips_updated_df_nodup = trips_updated_df.drop_duplicates([\"tripid\"], keep=\"first\")\n",
    "print(trips_updated_df_nodup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check\n",
    "print(\"trips DF shape: {}\".format(trips_prev_df_nodup.shape))\n",
    "print(\"trips_update DF shape: {}\".format(trips_updated_df_nodup.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Merge all + updated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Merge all + updated\n",
    "trips_df_all = pd.concat([trips_prev_df_nodup, trips_updated_df_nodup], sort=True)\n",
    "print(\"trips DF shape: {}\".format(trips_df_all.shape))\n",
    "\n",
    "trips_df_all[\"tripStartDate_formated\"] = pd.to_datetime(\n",
    "    trips_df_all[\"tripStartDate\"], unit=\"ms\"\n",
    ")\n",
    "trips_df_all[\"tripEndDate_formated\"] = pd.to_datetime(\n",
    "    trips_df_all[\"tripEndDate\"], unit=\"ms\"\n",
    ")\n",
    "\n",
    "# print('Remove duplicates...')\n",
    "# trips_df_nodup = trips_df.iloc[trips_df.astype(str).drop_duplicates(keep='first').index]\n",
    "# print('trips DF shape: {}'.format(trips_df.shape))\n",
    "print()\n",
    "print(\"Total trips: \", trips_df_all.shape[0])\n",
    "print(\"Total unique trips: \", len(trips_df_all[\"tripid\"].unique()))\n",
    "\n",
    "print()\n",
    "print(\"==> There are \", trips_df_all.shape[0], \"total Trips\")\n",
    "trips_df = trips_df_all[trips_df_all[\"tripStartDate_formated\"] >= CUTTING_DATE]\n",
    "print(\"==> There are \", trips_df.shape[0], \"Trips after\", CUTTING_DATE)\n",
    "\n",
    "# trips_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Take only trips also in trips_users_df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only trips in trips_users_df.\n",
    "# Filtering out trips before the cutting_date\n",
    "trips_df = trips_df[trips_df.tripid.isin(list(trips_users_df.tripid.unique()))]\n",
    "print(\"trips also in trips_users_df\", trips_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Preprocessing on Dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuration(d1, d2):\n",
    "\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "    d1 = datetime.strptime(str(d1)[0:19], fmt)\n",
    "    d2 = datetime.strptime(str(d2)[0:19], fmt)\n",
    "    duration_in_s = (d2 - d1).total_seconds()\n",
    "    #     minutes = divmod(duration_in_s, 60)[0]\n",
    "    minutes = divmod(duration_in_s, 60)\n",
    "    #     minutes = round(duration_in_s/60,2)\n",
    "    return minutes[0] + minutes[1] / 100\n",
    "\n",
    "\n",
    "trips_df[\"duration_formated_min\"] = trips_df.apply(\n",
    "    lambda x: getDuration(x[\"tripStartDate_formated\"], x[\"tripEndDate_formated\"]),\n",
    "    axis=1,\n",
    ")\n",
    "# trip start day\n",
    "trips_df[\"tripStartDay\"] = trips_df[\"tripStartDate_formated\"].dt.date\n",
    "\n",
    "# Remove trips with duration <=1\n",
    "trips_df = trips_df[trips_df[\"duration_formated_min\"] > 1]\n",
    "print(trips_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE\n",
    "trips_df.to_pickle(out_path + \"trips_df_original.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Extract objectives**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip_objs = []\n",
    "first = True\n",
    "for ix, ob in trips_df.iterrows():\n",
    "    objs = json_normalize([x for x in ob[\"objectives\"] if x is not None])\n",
    "    objs[\"tripid\"] = ob[\"tripid\"]\n",
    "    objs[\"tripStartDate_formated\"] = ob[\"tripStartDate_formated\"]\n",
    "    if first:\n",
    "        trip_objs = objs\n",
    "        first = False\n",
    "    else:\n",
    "        trip_objs = pd.concat([trip_objs, objs], sort=True)\n",
    "\n",
    "print(\"Initial shape:\", trip_objs.shape)\n",
    "\n",
    "# Convert names in English\n",
    "trip_obj_dict = {\n",
    "    \"0\": \"Home\",\n",
    "    \"1\": \"Work\",\n",
    "    \"2\": \"School_Education\",\n",
    "    \"3\": \"Everyday_Shopping\",\n",
    "    \"4\": \"Business_Trip\",\n",
    "    \"5\": \"Leisure_Hobby\",\n",
    "    \"6\": \"Pick_Up_Drop_Off\",\n",
    "    \"7\": \"Personal_Tasks_Errands\",\n",
    "    \"8\": \"Trip_Itself\",\n",
    "    \"9\": \"Other\",\n",
    "}\n",
    "\n",
    "trip_objs[\"tripObjectiveStringEN\"] = trip_objs[\"tripObjectiveCode\"].apply(\n",
    "    lambda x: trip_obj_dict[str(int(x))]\n",
    ")\n",
    "trip_objs.drop([\"tripObjectiveString\"], 1, inplace=True)\n",
    "\n",
    "# Merge 'School_Education' and 'Work'\n",
    "trip_objs[\"tripObjectiveStringEN\"] = trip_objs[\"tripObjectiveStringEN\"].apply(\n",
    "    lambda x: \"Work\" if x == \"School_Education\" else x\n",
    ")\n",
    "# Change 'Trip_Itself' into 'Other'\n",
    "trip_objs[\"tripObjectiveStringEN\"] = trip_objs[\"tripObjectiveStringEN\"].apply(\n",
    "    lambda x: \"Other\" if x == \"Trip_Itself\" else x\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"We know the Trip purpose of \",\n",
    "    len(trip_objs[\"tripid\"].unique()),\n",
    "    \" trips out of \",\n",
    "    trips_df.shape[0],\n",
    ")\n",
    "trip_objs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE\n",
    "trip_objs.to_pickle(out_path + \"trip_objs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group some objectives as agreed**\n",
    "\n",
    "- work (includes work and bisiness trip)\n",
    "- home\n",
    "- leisure (leisure and hobby)\n",
    "- shopping\n",
    "- education (school and education)\n",
    "- other (the rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work: work + business_trip\n",
    "trip_objs[\"tripObjectiveStringEN\"] = trip_objs[\"tripObjectiveStringEN\"].apply(\n",
    "    lambda x: \"Work\" if x == \"Business_Trip\" else x\n",
    ")\n",
    "# other\n",
    "other_lst = [\"Personal_Tasks_Errands\", \"Pick_Up_Drop_Off\"]\n",
    "trip_objs[\"tripObjectiveStringEN\"] = trip_objs[\"tripObjectiveStringEN\"].apply(\n",
    "    lambda x: \"Other\" if x in other_lst else x\n",
    ")\n",
    "\n",
    "# rename column\n",
    "trip_objs.rename(columns={\"tripObjectiveStringEN\": \"objective_str\"}, inplace=True)\n",
    "\n",
    "\n",
    "# save\n",
    "trip_objs.to_pickle(out_path + \"trip_objs_grouped.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all_legs'></a>\n",
    "### all_legs\n",
    "\n",
    "1. Extract legs from `trips_df`\n",
    "2. Preprocessing on legs\n",
    "3. Add category transport\n",
    "4. Add leg duration and preprocessing on duration and distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Extract legs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_original = []\n",
    "first = True\n",
    "for ix, trip in trips_df.iterrows():\n",
    "    legs = json_normalize([x for x in trip[\"legs\"] if x is not None])\n",
    "    legs[\"tripid\"] = trip[\"tripid\"]\n",
    "    legs[\"tripStartDate_formated\"] = trip[\"tripStartDate_formated\"]\n",
    "    if first:\n",
    "        all_legs_original = legs\n",
    "        first = False\n",
    "    else:\n",
    "        all_legs_original = pd.concat([all_legs_original, legs])\n",
    "print(all_legs_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE original\n",
    "all_legs_original.to_pickle(out_path + \"all_legs_original.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Preprocessing on legs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs = pd.read_pickle(out_path + \"all_legs_original.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial shape\", all_legs.shape)\n",
    "all_legs = all_legs.drop_duplicates([\"tripid\", \"legid\"], keep=\"first\")\n",
    "print()\n",
    "print(\"Remove rows with same tripid-legid:\", all_legs.shape)\n",
    "\n",
    "# Format date\n",
    "all_legs[\"startDate_formated\"] = pd.to_datetime(all_legs[\"startDate\"], unit=\"ms\")\n",
    "all_legs[\"endDate_formated\"] = pd.to_datetime(all_legs[\"endDate\"], unit=\"ms\")\n",
    "\n",
    "# Filter out legs performed before the cutting date\n",
    "all_legs = all_legs[all_legs[\"tripStartDate_formated\"] >= CUTTING_DATE]\n",
    "print()\n",
    "print(\"Total legs after the cutting date: \", all_legs.shape[0])\n",
    "\n",
    "# As suggested by Luis remove columns\n",
    "# RelaxingFactor\n",
    "# RelaxingFactorsText\n",
    "# ProductiveFactorsText\n",
    "# ProductiveFactors\n",
    "print()\n",
    "all_legs = all_legs.drop(\n",
    "    [\n",
    "        \"ProductiveFactors\",\n",
    "        \"ProductiveFactorsText\",\n",
    "        \"RelaxingFactors\",\n",
    "        \"RelaxingFactorsText\",\n",
    "    ],\n",
    "    1,\n",
    ")\n",
    "print(\"Remove 4 columns, Luis suggestion..\", all_legs.shape)\n",
    "\n",
    "# Remove legs with correctedModeOfTransport == -1\n",
    "all_legs = all_legs[all_legs[\"correctedModeOfTransport\"] != -1]\n",
    "print()\n",
    "print(\"Remove legs with correctedModeOfTransport == -1\", all_legs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on average speed.\n",
    "\n",
    "transp_mode_dict = {\n",
    "    \"0\": {\"name\": \"vehicle\", \"max_speed\": 200},\n",
    "    \"1\": {\"name\": \"bicycle\", \"max_speed\": 100},\n",
    "    \"2\": {\"name\": \"onfoot\", \"max_speed\": 12},\n",
    "    \"3\": {\"name\": \"still\", \"max_speed\": 0},\n",
    "    \"4\": {\"name\": \"unknown\", \"max_speed\": 100},\n",
    "    \"5\": {\"name\": \"tilting\", \"max_speed\": 100},\n",
    "    \"6\": {\"name\": \"inexistent\", \"max_speed\": 100},\n",
    "    \"7\": {\"name\": \"walking\", \"max_speed\": 12},\n",
    "    \"8\": {\"name\": \"running\", \"max_speed\": 20},\n",
    "    \"9\": {\"name\": \"car driver\", \"max_speed\": 250},\n",
    "    \"10\": {\"name\": \"train\", \"max_speed\": 350},\n",
    "    \"11\": {\"name\": \"tram\", \"max_speed\": 100},\n",
    "    \"12\": {\"name\": \"subway\", \"max_speed\": 100},\n",
    "    \"13\": {\"name\": \"ferry\", \"max_speed\": 200},\n",
    "    \"14\": {\"name\": \"plane\", \"max_speed\": 7000},\n",
    "    \"15\": {\"name\": \"bus\", \"max_speed\": 150},\n",
    "    \"16\": {\"name\": \"electricBike\", \"max_speed\": 50},\n",
    "    \"17\": {\"name\": \"bikeSharing\", \"max_speed\": 50},\n",
    "    \"18\": {\"name\": \"microScooter\", \"max_speed\": 50},\n",
    "    \"19\": {\"name\": \"skate\", \"max_speed\": 20},\n",
    "    \"20\": {\"name\": \"motorcycle\", \"max_speed\": 300},\n",
    "    \"21\": {\"name\": \"moped\", \"max_speed\": 80},\n",
    "    \"22\": {\"name\": \"carPassenger\", \"max_speed\": 250},\n",
    "    \"23\": {\"name\": \"taxi\", \"max_speed\": 250},\n",
    "    \"24\": {\"name\": \"rideHailing\", \"max_speed\": 100},\n",
    "    \"25\": {\"name\": \"carSharing\", \"max_speed\": 250},\n",
    "    \"26\": {\"name\": \"carpooling\", \"max_speed\": 250},\n",
    "    \"27\": {\"name\": \"busLongDistance\", \"max_speed\": 150},\n",
    "    \"28\": {\"name\": \"highSpeedTrain\", \"max_speed\": 350},\n",
    "    \"29\": {\"name\": \"other\", \"max_speed\": 100},\n",
    "    \"30\": {\"name\": \"otherPublic\", \"max_speed\": 300},\n",
    "    \"31\": {\"name\": \"otherActive\", \"max_speed\": 30},\n",
    "    \"32\": {\"name\": \"otherPrivate\", \"max_speed\": 250},\n",
    "    \"33\": {\"name\": \"intercityTrain\", \"max_speed\": 300},\n",
    "    \"34\": {\"name\": \"wheelChair\", \"max_speed\": 10},\n",
    "    \"35\": {\"name\": \"cargoBike\", \"max_speed\": 30},\n",
    "    \"36\": {\"name\": \"carSharingPassenger\", \"max_speed\": 250},\n",
    "    \"37\": {\"name\": \"electricWheelchair\", \"max_speed\": 30},\n",
    "}\n",
    "\n",
    "# put 4 as the code for unknown transport mode\n",
    "all_legs[[\"correctedModeOfTransport\"]] = all_legs[[\"correctedModeOfTransport\"]].fillna(\n",
    "    value=4\n",
    ")\n",
    "# check the avg speed, True if < than the max_speed (?)\n",
    "all_legs[\"check_avg_speed\"] = all_legs.apply(\n",
    "    lambda x: x[\"averageSpeed\"]\n",
    "    <= transp_mode_dict[str(int(x[\"correctedModeOfTransport\"]))][\"max_speed\"],\n",
    "    axis=1,\n",
    ")\n",
    "print(all_legs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Add transport category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mode of transport dict\n",
    "trasp_mode = pd.read_csv(meta_data_path + \"transport_mode.csv\", sep=\";\")\n",
    "trasp_mode_dict = trasp_mode.set_index(\"transport_code\").to_dict()[\"transport_str\"]\n",
    "\n",
    "# create transport category according to spreadsheet\n",
    "category_transp_mode_dict = {\n",
    "    \"walking\": [2, 7, 8, 34, 37],\n",
    "    \"cycling_emerging_micromobility\": [1, 16, 17, 18, 19, 31, 35],\n",
    "    \"public_transp_short_dist\": [10, 11, 12, 15, 30],\n",
    "    \"public_transp_long_dist\": [14, 13, 28, 33, 27],\n",
    "    \"private_motorized\": [0, 9, 20, 21, 22, 23, 25, 26, 32, 36],\n",
    "}\n",
    "#'Other_Private_Motorised':[32, 21, 20],\n",
    "#'private_motorized_driver':[0,9,25,26],\n",
    "#'private_motorized_passenger':[22, 23, 36]}\n",
    "\n",
    "# save category_transp_mode_dict\n",
    "with open(out_path + \"category_transp_mode_dict.json\", \"w\") as f:\n",
    "    json.dump(category_transp_mode_dict, f)\n",
    "\n",
    "# inverted category transp\n",
    "inverted_category_transp_mode_dict = dict(\n",
    "    (v, k) for k in category_transp_mode_dict for v in category_transp_mode_dict[k]\n",
    ")\n",
    "all_legs[\"transp_category\"] = all_legs.apply(\n",
    "    lambda x: inverted_category_transp_mode_dict.get(\n",
    "        int(x[\"correctedModeOfTransport\"])\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "all_legs[\"correctedModeOfTransport_str\"] = all_legs[\"correctedModeOfTransport\"].apply(\n",
    "    lambda x: trasp_mode_dict[x]\n",
    ")\n",
    "\n",
    "# Remove transport category 'still'\n",
    "all_legs = all_legs[all_legs[\"correctedModeOfTransport_str\"] != \"still\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Add leg duration and preprocessing on duration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute leg duration (in minutes)\n",
    "print(\"Compute leg duration in minutes\")\n",
    "print()\n",
    "all_legs[\"legDuration\"] = all_legs[\"legDuration\"].fillna(0)\n",
    "all_legs[\"legDuration_formated_min\"] = all_legs[\"legDuration\"].apply(lambda x: x / 60)\n",
    "\n",
    "\n",
    "def getDuration(d1, d2):\n",
    "\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "    d1 = datetime.strptime(str(d1)[0:19], fmt)\n",
    "    d2 = datetime.strptime(str(d2)[0:19], fmt)\n",
    "    duration_in_s = (d2 - d1).total_seconds()\n",
    "    #     minutes = divmod(duration_in_s, 60)[0]\n",
    "    minutes = divmod(duration_in_s, 60)\n",
    "    #     minutes = round(duration_in_s/60,2)\n",
    "    return minutes[0] + minutes[1] / 100\n",
    "\n",
    "\n",
    "all_legs[\"inferred_leg_duration_min\"] = all_legs.apply(\n",
    "    lambda x: getDuration(x[\"startDate_formated\"], x[\"endDate_formated\"]), axis=1\n",
    ")\n",
    "all_legs[\"legStartDay\"] = pd.to_datetime(all_legs[\"startDate_formated\"]).dt.date\n",
    "\n",
    "# Remove legs with duration <= 1\n",
    "all_legs = all_legs[((all_legs[\"inferred_leg_duration_min\"] > 1))]\n",
    "print(\"Remove legs with duration <= 1\", all_legs.shape)\n",
    "\n",
    "# Remove legs with distance > 1M meters (1000km) with transport != train\n",
    "all_legs = all_legs[\n",
    "    (all_legs[\"correctedModeOfTransport_str\"] != \"train\")\n",
    "    | (all_legs[\"legDistance\"] < 1000000)\n",
    "]\n",
    "print(\"Remove legs with distance > 1M meters (1000km)\", all_legs.shape)\n",
    "\n",
    "# Remove legs that last more than 2 days (?)\n",
    "# ex. all_legs[all_legs.legid == '#22:6670']\n",
    "print(\n",
    "    \"Trips that last more than 2 days:\",\n",
    "    len(all_legs[all_legs[\"inferred_leg_duration_min\"] > 60 * 48]),\n",
    ")\n",
    "\n",
    "# Remove cases in which the same trip is stored with different id\n",
    "\n",
    "all_legs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE\n",
    "all_legs.to_pickle(out_path + \"all_legs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all_legs_final_ds_user_info'></a>\n",
    "### all_legs_final_ds_user_info\n",
    "\n",
    "1. Merge legs info with users info and trip info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of trips: \", len(all_legs[\"tripid\"].unique()))\n",
    "print(\"Number of legs: \", len(all_legs[\"legid\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# trips_users_df: link between users and trips\n",
    "# users_df_with_trips: demographic info of users\n",
    "\n",
    "# add to each trip the demographic info of users --> trips_users_df_temp_1\n",
    "trips_users_df_temp = trips_users_df[[\"tripid\", \"userid\"]]\n",
    "trips_users_df_temp_1 = pd.merge(\n",
    "    trips_users_df_temp, users_df_with_trips, on=\"userid\", how=\"left\"\n",
    ")\n",
    "\n",
    "# merge legs and trips+user\n",
    "all_legs_final_ds_user_info = pd.merge(\n",
    "    all_legs, trips_users_df_temp_1, on=\"tripid\", how=\"left\"\n",
    ")\n",
    "\n",
    "# add info day\n",
    "all_legs_final_ds_user_info[\"weekday\"] = all_legs_final_ds_user_info[\n",
    "    \"legStartDay\"\n",
    "].apply(lambda x: calendar.day_name[x.weekday()])\n",
    "\n",
    "we_vs_wd = {\"weekend\": [\"Saturday\", \"Sunday\"]}\n",
    "all_legs_final_ds_user_info[\"we_vs_wd\"] = all_legs_final_ds_user_info.apply(\n",
    "    lambda x: \"Weekend\" if x[\"weekday\"] in we_vs_wd[\"weekend\"] else \"Working_day\",\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allcols = list(all_legs_final_ds_user_info.columns)\n",
    "colstoremove = [\n",
    "    \"legid\",\n",
    "    \"tripid\",\n",
    "    \"onCampaigns\",\n",
    "    \"registerTimestamp\",\n",
    "    \"userSettings.actValue\",\n",
    "    \"userSettings.city\",\n",
    "    \"userSettings.country\",\n",
    "    \"userSettings.degree\",\n",
    "    \"userSettings.gender\",\n",
    "    \"userSettings.hasSetMobilityGoal\",\n",
    "    \"userSettings.labourStatusHousehold\",\n",
    "    \"userSettings.lang\",\n",
    "    \"userSettings.maritalStatusHousehold\",\n",
    "    \"userSettings.maxAge\",\n",
    "    \"userSettings.minAge\",\n",
    "    \"userSettings.mobilityGoalChosen\",\n",
    "    \"userSettings.mobilityGoalPoints\",\n",
    "    \"userSettings.numberPeopleHousehold\",\n",
    "    \"userSettings.pointsPerCampaign\",\n",
    "    \"userSettings.preferedMots\",\n",
    "    \"userSettings.prodValue\",\n",
    "    \"userSettings.relValue\",\n",
    "    \"userSettings.yearsOfResidenceHousehold\",\n",
    "    \"userSettings.gender_en\",\n",
    "    \"userSettings.degree_en\",\n",
    "    \"maritalStatus\",\n",
    "    \"userSettings.labourStatusHousehold_en\",\n",
    "]\n",
    "# colsfordup = list(set(allcols) - set(colstoremove))\n",
    "# cols\n",
    "# print(all_legs_final_ds_user_info.shape)\n",
    "# all_legs_final_ds_user_info = all_legs_final_ds_user_info[colsfordup]\n",
    "# print(all_legs_final_ds_user_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_legs_final_ds_user_info.shape)\n",
    "all_legs_final_ds_user_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE\n",
    "all_legs_final_ds_user_info.to_pickle(out_path + \"all_legs_final_ds_user_info.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all_legs_merged'></a>\n",
    "### all_legs_merged\n",
    "\n",
    "Merge legs that belong to the same user and have the same mode of transport and the time between the two legs is less than 5 minutes.\n",
    "<br>Merged Variables:\n",
    "- end date: the end date formatted of the second leg\n",
    "- max speed: take the maximum of the two speeds\n",
    "- trueDistance: take the sum of the two distances\n",
    "- inferred_leg_duration_min: take the sum of the two distances\n",
    "- wastedTime: consider as wasted time the average of the WT of merged legs\n",
    "- wastedTime_withtime: compute also the worthwhileness in relation to travelled time.\n",
    "\n",
    "\n",
    "1. Merge legs\n",
    "2. Average Wasted Time for merged legs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Merge legs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info = pd.read_pickle(\n",
    "    out_path + \"all_legs_final_ds_user_info.pkl\"\n",
    ")\n",
    "print(all_legs_final_ds_user_info.shape)\n",
    "print(\n",
    "    \"Number of users with a leg: \", len(all_legs_final_ds_user_info[\"userid\"].unique())\n",
    ")\n",
    "print(\n",
    "    \"Legs with duration < 5:\",\n",
    "    len(\n",
    "        all_legs_final_ds_user_info[\n",
    "            ((all_legs_final_ds_user_info[\"inferred_leg_duration_min\"] < 1))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Legs with class different from Leg:\",\n",
    "    len(all_legs_final_ds_user_info[all_legs_final_ds_user_info[\"class\"] != \"Leg\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info = pd.read_pickle(\n",
    "    out_path + \"all_legs_final_ds_user_info.pkl\"\n",
    ")\n",
    "print(all_legs_final_ds_user_info.shape)\n",
    "print(\n",
    "    \"Number of users with a leg: \", len(all_legs_final_ds_user_info[\"userid\"].unique())\n",
    ")\n",
    "print(\n",
    "    \"Legs with duration < 5:\",\n",
    "    len(\n",
    "        all_legs_final_ds_user_info[\n",
    "            ((all_legs_final_ds_user_info[\"inferred_leg_duration_min\"] < 1))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Legs with class different from Leg:\",\n",
    "    len(all_legs_final_ds_user_info[all_legs_final_ds_user_info[\"class\"] != \"Leg\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_final_ds_user_info.groupby(\"class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDuration(d1, d2):\n",
    "    fmt = \"%Y-%m-%d %H:%M:%S\"\n",
    "    d1 = datetime.strptime(str(d1)[0:19], fmt)\n",
    "    d2 = datetime.strptime(str(d2)[0:19], fmt)\n",
    "    duration_in_s = (d2 - d1).total_seconds()\n",
    "    minutes = divmod(duration_in_s, 60)\n",
    "    return minutes[0] + minutes[1] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "all_legs_for_merging = all_legs_final_ds_user_info.copy()\n",
    "# dictionary to keep track of merged legs final_legid: [list of merged legs id]\n",
    "merged_legs = {}\n",
    "cols = list(all_legs_for_merging.columns)\n",
    "all_legs_merged = pd.DataFrame(columns=cols)\n",
    "\n",
    "uix = 1\n",
    "for user in all_legs_for_merging[\"userid\"].unique():\n",
    "    # print(uix)\n",
    "    # user = 'o3QNooVleBR9WfbIaCqsaj3DlDl2'\n",
    "    uix += 1\n",
    "    first = True\n",
    "    curruser_legs = all_legs_for_merging[\n",
    "        all_legs_for_merging[\"userid\"] == user\n",
    "    ].sort_values(\"startDate_formated\", ascending=True)\n",
    "\n",
    "    # if the user has only a leg\n",
    "    if curruser_legs.shape[0] == 1:\n",
    "        all_legs_merged = pd.concat([all_legs_merged, pd.DataFrame(curruser_legs)])\n",
    "    else:\n",
    "        # for each leg (except the last one) get the consecutive ones\n",
    "        for leg_ix in range(curruser_legs.shape[0] - 1):\n",
    "            # print(currleg.index[leg_ix])\n",
    "            currleg = curruser_legs.iloc[\n",
    "                leg_ix : leg_ix + 1,\n",
    "            ]\n",
    "            currleg_s = curruser_legs.iloc[\n",
    "                leg_ix,\n",
    "            ]\n",
    "            nextleg = curruser_legs.iloc[\n",
    "                leg_ix + 1 : leg_ix + 2,\n",
    "            ]\n",
    "            nextleg_s = curruser_legs.iloc[\n",
    "                leg_ix + 1,\n",
    "            ]\n",
    "\n",
    "            if first:\n",
    "                # print('first')\n",
    "                all_legs_merged = pd.concat([all_legs_merged, pd.DataFrame(currleg)])\n",
    "                first = False\n",
    "            # if same transport mode\n",
    "            if (\n",
    "                curruser_legs.iloc[leg_ix,][\"correctedModeOfTransport_str\"]\n",
    "                == curruser_legs.iloc[leg_ix + 1,][\"correctedModeOfTransport_str\"]\n",
    "            ):\n",
    "                # get the duration\n",
    "                deltatime = getDuration(\n",
    "                    curruser_legs.iloc[leg_ix + 1,][\"startDate_formated\"],\n",
    "                    curruser_legs.iloc[leg_ix,][\"endDate_formated\"],\n",
    "                )\n",
    "                # MERGE if the duration is < 5\n",
    "                if np.abs(deltatime) < 5:\n",
    "                    # append to the last position of all_legs_merged dataset the merged leg\n",
    "\n",
    "                    all_legs_merged.iloc[\n",
    "                        -1, all_legs_merged.columns.get_loc(\"endDate_formated\")\n",
    "                    ] = nextleg_s[\"endDate_formated\"]\n",
    "                    all_legs_merged.iloc[\n",
    "                        -1, all_legs_merged.columns.get_loc(\"maxSpeed\")\n",
    "                    ] = max(\n",
    "                        all_legs_merged.iloc[\n",
    "                            -1, all_legs_merged.columns.get_loc(\"maxSpeed\")\n",
    "                        ],\n",
    "                        nextleg_s[\"maxSpeed\"],\n",
    "                    )\n",
    "                    all_legs_merged.iloc[\n",
    "                        -1, all_legs_merged.columns.get_loc(\"trueDistance\")\n",
    "                    ] = (\n",
    "                        all_legs_merged.iloc[\n",
    "                            -1, all_legs_merged.columns.get_loc(\"trueDistance\")\n",
    "                        ]\n",
    "                        + nextleg_s[\"trueDistance\"]\n",
    "                    )\n",
    "                    all_legs_merged.iloc[\n",
    "                        -1, all_legs_merged.columns.get_loc(\"inferred_leg_duration_min\")\n",
    "                    ] = (\n",
    "                        all_legs_merged.iloc[\n",
    "                            -1,\n",
    "                            all_legs_merged.columns.get_loc(\n",
    "                                \"inferred_leg_duration_min\"\n",
    "                            ),\n",
    "                        ]\n",
    "                        + nextleg_s[\"inferred_leg_duration_min\"]\n",
    "                    )\n",
    "\n",
    "                    # create the dictionary\n",
    "                    try:\n",
    "                        merged_legs[\n",
    "                            all_legs_merged.iloc[\n",
    "                                -1, all_legs_merged.columns.get_loc(\"legid\")\n",
    "                            ]\n",
    "                        ].append(nextleg_s[\"legid\"])\n",
    "                    except:\n",
    "                        merged_legs[\n",
    "                            all_legs_merged.iloc[\n",
    "                                -1, all_legs_merged.columns.get_loc(\"legid\")\n",
    "                            ]\n",
    "                        ] = []\n",
    "                        merged_legs[\n",
    "                            all_legs_merged.iloc[\n",
    "                                -1, all_legs_merged.columns.get_loc(\"legid\")\n",
    "                            ]\n",
    "                        ].append(nextleg_s[\"legid\"])\n",
    "\n",
    "                else:  # if duration > 5\n",
    "\n",
    "                    all_legs_merged = pd.concat(\n",
    "                        [all_legs_merged, pd.DataFrame(nextleg)]\n",
    "                    )\n",
    "\n",
    "            else:  # if different transport mode\n",
    "                all_legs_merged = pd.concat([all_legs_merged, pd.DataFrame(nextleg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "all_legs_merged.to_pickle(out_path + \"all_legs_merged.pkl\")\n",
    "with open(out_path + \"merged_legs.json\", \"w\") as file:\n",
    "    file.write(json.dumps(merged_legs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Average Wasted Time for merged legs**\n",
    "\n",
    "For the merged legs add the columns:\n",
    "\n",
    "- `wastedTime`: consider as wasted time the average of the WT of merged legs\n",
    "- `wastedTime_withtime`: compute also the worthwhileness in relation to travelled time. Given 2 legs l1 and l2, to obtain the related wasted times we compute $((l1*t1)+(l2*t2))/(t1+t2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "all_legs_merged = pd.read_pickle(out_path + \"all_legs_merged.pkl\")\n",
    "\n",
    "with open(out_path + \"merged_legs.json\") as json_file:\n",
    "    merged_legs = json.load(json_file)\n",
    "\n",
    "print(\"Merged legs:\", len(all_legs_merged))\n",
    "print(\"Different trips:\", len(all_legs_merged.tripid.unique()))\n",
    "print(\"Users:\", len(all_legs_merged.userid.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_legs_merged[\"tot_wastedTime\"] = 0\n",
    "all_legs_merged[\"count_wastedTime\"] = 0\n",
    "all_legs_merged[\"wastedTime_withtime\"] = all_legs_merged[\"wastedTime\"]\n",
    "\n",
    "tot_w = 0\n",
    "count_w = 0\n",
    "\n",
    "tot_speed = 0\n",
    "count_speed = 0\n",
    "\n",
    "tot_wt = 0\n",
    "count_wt = 0\n",
    "\n",
    "for k_legid, v_mergedlegs in merged_legs.items():\n",
    "    # print(k_legid)\n",
    "    ## Merged leg\n",
    "    # take the average speed\n",
    "    tot_speed = all_legs_final_ds_user_info[\n",
    "        all_legs_final_ds_user_info[\"legid\"] == k_legid\n",
    "    ].T.squeeze()[\"averageSpeed\"]\n",
    "    count_speed = 1\n",
    "    # take the WT\n",
    "    curr_w = all_legs_final_ds_user_info[\n",
    "        all_legs_final_ds_user_info[\"legid\"] == k_legid\n",
    "    ].T.squeeze()[\"wastedTime\"]\n",
    "    # take the duration of current leg\n",
    "    curr_time = all_legs_final_ds_user_info[\n",
    "        all_legs_final_ds_user_info[\"legid\"] == k_legid\n",
    "    ].T.squeeze()[\"inferred_leg_duration_min\"]\n",
    "\n",
    "    if curr_w >= 1 and curr_w <= 5:\n",
    "        tot_w = curr_w\n",
    "        count_w = 1\n",
    "\n",
    "        # we want to compute also the worthwhilness in relation to travelled time\n",
    "        # so given 2 legs l1 and l2 we compute ((l1*t1)+(l2*t2))/t1+t2\n",
    "        tot_wt = curr_w * curr_time\n",
    "        count_wt = curr_time\n",
    "\n",
    "    for l in v_mergedlegs:\n",
    "\n",
    "        tot_speed += all_legs_final_ds_user_info[\n",
    "            all_legs_final_ds_user_info[\"legid\"] == l\n",
    "        ].T.squeeze()[\"averageSpeed\"]\n",
    "        count_speed += 1\n",
    "\n",
    "        curr_w = all_legs_final_ds_user_info[\n",
    "            all_legs_final_ds_user_info[\"legid\"] == l\n",
    "        ].T.squeeze()[\"wastedTime\"]\n",
    "        curr_time = all_legs_final_ds_user_info[\n",
    "            all_legs_final_ds_user_info[\"legid\"] == l\n",
    "        ].T.squeeze()[\"inferred_leg_duration_min\"]\n",
    "        if curr_w >= 1 and curr_w <= 5:\n",
    "            tot_w += curr_w\n",
    "            count_w += 1\n",
    "            # we want to compute Also the worthwhilness in relation to travelled time\n",
    "            # so given 2 legs l1 and l2 we compute ((l1*t1)+(l2*t2))/t1+t2\n",
    "            tot_wt += curr_w * curr_time\n",
    "            count_wt += curr_time\n",
    "\n",
    "    all_legs_merged.loc[all_legs_merged[\"legid\"] == k_legid, [\"wastedTime\"]] = (\n",
    "        tot_w / count_w\n",
    "    )  # average WT of merged legs\n",
    "    all_legs_merged.loc[all_legs_merged[\"legid\"] == k_legid, [\"tot_wastedTime\"]] = tot_w\n",
    "    all_legs_merged.loc[\n",
    "        all_legs_merged[\"legid\"] == k_legid, [\"count_wastedTime\"]\n",
    "    ] = count_w\n",
    "\n",
    "    try:\n",
    "        all_legs_merged.loc[\n",
    "            all_legs_merged[\"legid\"] == k_legid, [\"wastedTime_withtime\"]\n",
    "        ] = (tot_wt / count_wt)\n",
    "    except:\n",
    "        print(v_mergedlegs)\n",
    "        break\n",
    "\n",
    "    all_legs_merged.loc[all_legs_merged[\"legid\"] == k_legid, [\"averageSpeed\"]] = (\n",
    "        tot_speed / count_speed\n",
    "    )\n",
    "\n",
    "all_legs_merged.to_pickle(out_path + \"all_legs_merged_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all_factors'></a>\n",
    "### all_factors\n",
    "\n",
    "For each leg there are some factors that can have positive or negative (or both) influence.\n",
    "They can be divided into:\n",
    "- **GT:** gettingThereFactors\n",
    "- **ACT:** activitiesFactors\n",
    "- **WYR:** whileYouRideFactors\n",
    "- **CP:** comfortAndPleasentFactors\n",
    "\n",
    "-----------\n",
    "\n",
    "1. Select factors from each leg\n",
    "2. Select generic activities from each leg\n",
    "3. Create the Encodings\n",
    "4. Map the encoded factors\n",
    "5. Check not encoded factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select factors from each leg**\n",
    "<br>**2. Select generic activities from each leg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = []\n",
    "all_gen_act = []\n",
    "first = True\n",
    "i = 0\n",
    "for index, leg in all_legs_merged.iterrows():\n",
    "\n",
    "    gt_factors = pd.DataFrame(list(leg[\"gettingThereFactors\"]))\n",
    "    gt_factors[\"type\"] = \"GT\"\n",
    "\n",
    "    act_fact = pd.DataFrame(list(leg[\"activitiesFactors\"]))\n",
    "    act_fact[\"type\"] = \"ACT\"\n",
    "    curr_factors = pd.concat([gt_factors, act_fact])\n",
    "\n",
    "    wyr_fact = pd.DataFrame(list(leg[\"whileYouRideFactors\"]))\n",
    "    wyr_fact[\"type\"] = \"WYR\"\n",
    "    curr_factors = pd.concat([curr_factors, wyr_fact])\n",
    "\n",
    "    cp_fact = pd.DataFrame(list(leg[\"comfortAndPleasentFactors\"]))\n",
    "    cp_fact[\"type\"] = \"CP\"\n",
    "    curr_factors = pd.concat([curr_factors, cp_fact])\n",
    "\n",
    "    curr_factors[\"tripid\"] = leg[\"tripid\"]\n",
    "    curr_factors[\"legid\"] = leg[\"legid\"]\n",
    "    curr_factors[\"correctedModeOfTransport\"] = leg[\"correctedModeOfTransport\"]\n",
    "    curr_factors[\"correctedModeOfTransport_str\"] = leg[\"correctedModeOfTransport_str\"]\n",
    "    curr_factors[\"legStartDay\"] = leg[\"legStartDay\"]\n",
    "\n",
    "    ### ?\n",
    "    gen_act = pd.DataFrame(list(leg[\"genericActivities\"]))\n",
    "    gen_act[\"tripid\"] = leg[\"tripid\"]\n",
    "    gen_act[\"legid\"] = leg[\"legid\"]\n",
    "    gen_act[\"legStartDay\"] = leg[\"legStartDay\"]\n",
    "\n",
    "    if first:\n",
    "        all_factors = curr_factors\n",
    "        all_gen_act = gen_act\n",
    "        first = False\n",
    "    else:\n",
    "        all_factors = pd.concat([all_factors, curr_factors])\n",
    "        all_gen_act = pd.concat([all_gen_act, gen_act])\n",
    "\n",
    "\n",
    "all_factors.reset_index(inplace=True)\n",
    "all_factors.drop(\"index\", 1)\n",
    "print(\"Number of legs containing factors\", len(all_factors[\"legid\"].unique()))\n",
    "print(\"Remember: a leg may contain multiple factors\\n\")\n",
    "print(\"All factors:\", all_factors.shape)\n",
    "print()\n",
    "all_gen_act = all_gen_act.reset_index()\n",
    "all_gen_act = all_gen_act.drop(\"text\", 1)\n",
    "print(\"all_gen_act:\", all_gen_act.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## SAVE\n",
    "all_factors.to_pickle(out_path + \"all_factors_original.pkl\")\n",
    "# all_gen_act.to_pickle(out_path + \"all_gen_act.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Select generic activities from each leg**\n",
    "\n",
    "- If the activity is driving and the TC is walking, change into **walking**.\n",
    "- If the activity is driving and the TC is cycling, change into **cycling**.\n",
    "- If the activity is driving and the TC is public transport, **drop the leg**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "all_legs_merged = pd.read_pickle(out_path + \"all_legs_merged_1.pkl\")\n",
    "\n",
    "all_gen_act = pd.read_pickle(out_path + \"all_gen_act.pkl\")\n",
    "# rename values in  column 'code'\n",
    "all_gen_act[\"code\"] = all_gen_act[\"code\"].apply(lambda x: x[10:])\n",
    "\n",
    "# add transp_category\n",
    "all_gen_act = all_gen_act.merge(\n",
    "    all_legs_merged[[\"legid\", \"transp_category\"]], on=\"legid\"\n",
    ").drop_duplicates()\n",
    "\n",
    "# convert walking and cycling\n",
    "def convert_driving(row):\n",
    "    if row[\"transp_category\"] == \"walking\" and row[\"code\"] == \"Driving\":\n",
    "        return \"Walking\"\n",
    "    if (\n",
    "        row[\"transp_category\"] == \"cycling_emerging_micromobility\"\n",
    "        and row[\"code\"] == \"Driving\"\n",
    "    ):\n",
    "        return \"Cycling\"\n",
    "    else:\n",
    "        return row[\"code\"]\n",
    "\n",
    "\n",
    "all_gen_act[\"code\"] = all_gen_act.apply(lambda row: convert_driving(row), axis=1)\n",
    "\n",
    "# drop if public transport -- 6 legs!!\n",
    "xx = all_gen_act[\n",
    "    (\n",
    "        all_gen_act.transp_category.isin(\n",
    "            [\"public_transp_short_dist\", \"public_transp_long_dist\"]\n",
    "        )\n",
    "    )\n",
    "    & (all_gen_act.code == \"Driving\")\n",
    "]\n",
    "print(\"Legs with driving and public transport: \", len(xx))\n",
    "print(all_gen_act.shape)\n",
    "all_gen_act = all_gen_act[\n",
    "    ~(\n",
    "        (\n",
    "            all_gen_act.transp_category.isin(\n",
    "                [\"public_transp_short_dist\", \"public_transp_long_dist\"]\n",
    "            )\n",
    "        )\n",
    "        & (all_gen_act.code == \"Driving\")\n",
    "    )\n",
    "]\n",
    "print(all_gen_act.shape)\n",
    "all_gen_act.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE\n",
    "all_gen_act.to_pickle(out_path + \"all_gen_act.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create the Encodings**\n",
    "\n",
    "There are two different encodings:\n",
    "\n",
    "- the one for the codes from 0 to 11 depending on the transport category and on the type of activity\n",
    "- the one for the cosed from 1001 to 3212\n",
    "\n",
    "We created also the encodings for the codes that are actually strings.\n",
    "\n",
    "**oss:** in `all_factors` there are also codes with strings that need to be encoded/preprocessed\n",
    "<br> **oss:** in the encoding files there is a mistake. `Crowdness_eating` becomes `Crowdness_Seating`\n",
    "<br>**oss:** in the encoding files the transport category 'carDriver' is 'car'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = pd.read_pickle(out_path + \"all_factors_original.pkl\")\n",
    "all_factors.drop(\"index\", 1, inplace=True)\n",
    "all_factors.reset_index(inplace=True)\n",
    "all_factors.drop(\"index\", 1, inplace=True)\n",
    "print(\"All factors shape:\", all_factors.shape)\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors.groupby(\"code\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# For each type of factors read the csv with the encoding\n",
    "\n",
    "\n",
    "##GettingThereFactors\n",
    "activeTranspGTFactors = pd.read_csv(\n",
    "    meta_data_path + \"activeTranspGTFactors.csv\", sep=\";\"\n",
    ")\n",
    "privateTransportGTFactors = pd.read_csv(\n",
    "    meta_data_path + \"PrivateTransportGTFactors.csv\", sep=\";\"\n",
    ")\n",
    "publicTransportGTFactors = pd.read_csv(\n",
    "    meta_data_path + \"PublicTransportGTFactors.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "GTFactors = pd.concat(\n",
    "    [activeTranspGTFactors, privateTransportGTFactors, publicTransportGTFactors]\n",
    ")\n",
    "GTFactors[\"mode_activity\"] = GTFactors[\"mode_activity\"].apply(\n",
    "    lambda x: \"carDriver\" if x == \"car\" else x\n",
    ")\n",
    "GTFactors[\"factor_k\"] = GTFactors[\"factor_k\"].apply(\n",
    "    lambda x: \"Crowdedness_Seating\" if x == \"Crowdedness_eating\" else x\n",
    ")\n",
    "\n",
    "# for each transport category create a dictionary with code: factors\n",
    "# transport category : {code: factors}\n",
    "gt_fact = {}\n",
    "for index, fac in GTFactors.iterrows():\n",
    "    try:\n",
    "        gt_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "    except:\n",
    "        gt_fact[fac[\"mode_activity\"]] = {}\n",
    "        gt_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "##ACTFactors\n",
    "activeTransportACTFactors = pd.read_csv(\n",
    "    meta_data_path + \"ActiveTransportACTFactors.csv\", sep=\";\"\n",
    ")\n",
    "privateTransportACTFactors = pd.read_csv(\n",
    "    meta_data_path + \"PrivateTransportACTFactors.csv\", sep=\";\"\n",
    ")\n",
    "publicTransportACTFactors = pd.read_csv(\n",
    "    meta_data_path + \"PublicTransportACTFactors.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "ACT_factors = pd.concat(\n",
    "    [activeTransportACTFactors, privateTransportACTFactors, publicTransportACTFactors]\n",
    ")\n",
    "ACT_factors[\"mode_activity\"] = ACT_factors[\"mode_activity\"].apply(\n",
    "    lambda x: \"carDriver\" if x == \"car\" else x\n",
    ")\n",
    "ACT_factors[\"factor_k\"] = ACT_factors[\"factor_k\"].apply(\n",
    "    lambda x: \"Crowdedness_Seating\" if x == \"Crowdedness_eating\" else x\n",
    ")\n",
    "\n",
    "act_fact = {}\n",
    "for index, fac in ACT_factors.iterrows():\n",
    "    try:\n",
    "        act_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "    except:\n",
    "        act_fact[fac[\"mode_activity\"]] = {}\n",
    "        act_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "##WYRFactors\n",
    "activeTransportWYRFactors = pd.read_csv(\n",
    "    meta_data_path + \"ActiveTransportWYRFactors.csv\", sep=\";\"\n",
    ")\n",
    "privateTransportWYRFactors = pd.read_csv(\n",
    "    meta_data_path + \"PrivateTransportWYRFactors.csv\", sep=\";\"\n",
    ")\n",
    "publicTransportWYRFactors = pd.read_csv(\n",
    "    meta_data_path + \"PublicTransportWYRFactors.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "WYR_factors = pd.concat(\n",
    "    [activeTransportWYRFactors, privateTransportWYRFactors, publicTransportWYRFactors]\n",
    ")\n",
    "WYR_factors[\"mode_activity\"] = WYR_factors[\"mode_activity\"].apply(lambda x: x.strip())\n",
    "WYR_factors[\"mode_activity\"] = WYR_factors[\"mode_activity\"].apply(\n",
    "    lambda x: \"carDriver\" if x == \"car\" else x\n",
    ")\n",
    "WYR_factors[\"factor_k\"] = WYR_factors[\"factor_k\"].apply(\n",
    "    lambda x: \"Crowdedness_Seating\" if x == \"Crowdedness_eating\" else x\n",
    ")\n",
    "\n",
    "wyr_fact = {}\n",
    "for index, fac in WYR_factors.iterrows():\n",
    "    try:\n",
    "        wyr_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "    except:\n",
    "        wyr_fact[fac[\"mode_activity\"]] = {}\n",
    "        wyr_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "\n",
    "\n",
    "# ============================================\n",
    "##CPFactors\n",
    "publicTransportCPFactors = pd.read_csv(\n",
    "    meta_data_path + \"PublicTransportCPFactors.csv\", sep=\";\"\n",
    ")\n",
    "privateTransportCPFactors = pd.read_csv(\n",
    "    meta_data_path + \"PrivateTransportCPFactors.csv\", sep=\";\"\n",
    ")\n",
    "activeTransportCPFactors = pd.read_csv(\n",
    "    meta_data_path + \"ActiveTransportCPFactors.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "CP_factors = pd.concat(\n",
    "    [activeTransportCPFactors, publicTransportCPFactors, privateTransportCPFactors]\n",
    ")\n",
    "CP_factors[\"mode_activity\"] = CP_factors[\"mode_activity\"].apply(\n",
    "    lambda x: \"carDriver\" if x == \"car\" else x\n",
    ")\n",
    "CP_factors[\"factor_k\"] = CP_factors[\"factor_k\"].apply(\n",
    "    lambda x: \"Crowdedness_Seating\" if x == \"Crowdedness_eating\" else x\n",
    ")\n",
    "\n",
    "cp_fact = {}\n",
    "for index, fac in CP_factors.iterrows():\n",
    "    try:\n",
    "        cp_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "    except:\n",
    "        cp_fact[fac[\"mode_activity\"]] = {}\n",
    "        cp_fact[fac[\"mode_activity\"]][fac[\"code\"]] = fac[\"factor_k\"]\n",
    "\n",
    "\n",
    "### Experience Factors\n",
    "# {type: {transport_category : {code: factor}}}\n",
    "# {'GT': {'walking': {0: 'Simplicity_Difficulty_Of_The_Route', ...\n",
    "\n",
    "exp_fact = {}\n",
    "exp_fact[\"GT\"] = gt_fact\n",
    "exp_fact[\"ACT\"] = act_fact\n",
    "exp_fact[\"WYR\"] = wyr_fact\n",
    "exp_fact[\"CP\"] = cp_fact\n",
    "\n",
    "\n",
    "### Create a list with all the factors\n",
    "factor_list = set(GTFactors[\"factor_k\"].unique())\n",
    "factor_list = factor_list.union(set(ACT_factors[\"factor_k\"].unique()))\n",
    "factor_list = factor_list.union(set(WYR_factors[\"factor_k\"].unique()))\n",
    "factor_list = factor_list.union(set(CP_factors[\"factor_k\"].unique()))\n",
    "print(\"Total factors:\", len(factor_list))\n",
    "print()\n",
    "\n",
    "\n",
    "### Strings in the original all_factors.\n",
    "# Factors are all the same with the exception of\n",
    "# 'Ability_To_Do_The_Things_I_Want' and 'Space_onboard_for_luggage_pram_bicycle'\n",
    "str_codes_factors = all_factors[~all_factors[\"code\"].isin(np.arange(0, 5000))]\n",
    "str_factors_lst = set(str_codes_factors.code.unique())\n",
    "# change 'Ability_To_Do_The_Things_I_Want' in 'Ability_To_Do_What_I_Wanted'\n",
    "# change 'Space_onboard_for_luggage_pram_bicycle' in 'Space_Onboard_For_Lugagge_Pram_Bicycle'\n",
    "all_factors[\"code\"] = all_factors[\"code\"].apply(\n",
    "    lambda x: \"Ability_To_Do_What_I_Wanted\"\n",
    "    if x == \"Ability_To_Do_The_Things_I_Want\"\n",
    "    else x\n",
    ")\n",
    "all_factors[\"code\"] = all_factors[\"code\"].apply(\n",
    "    lambda x: \"Space_Onboard_For_Lugagge_Pram_Bicycle\"\n",
    "    if x == \"Space_onboard_for_luggage_pram_bicycle\"\n",
    "    else x\n",
    ")\n",
    "\n",
    "\n",
    "### New encoding\n",
    "new_fact_encoding = pd.read_csv(\n",
    "    meta_data_path + \"encoding-wip-no-colisions-of-IDs_NEW.csv\", sep=\";\"\n",
    ")\n",
    "\n",
    "new_fact = {}\n",
    "for index, fac in new_fact_encoding.iterrows():\n",
    "    new_fact[fac[\"code\"]] = fac[\"new_factor\"]\n",
    "\n",
    "print(\"Total new factors:\", len(set(new_fact_encoding.new_factor.unique())))\n",
    "new_factors = set(new_fact_encoding.new_factor.unique())\n",
    "all_factors_lst = new_factors.union(factor_list)\n",
    "\n",
    "new_fact_encoding.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Check intersection between encodings ---\")\n",
    "\n",
    "print(\"Factors only in the first encoding: \\n\", factor_list.difference(new_factors))\n",
    "print()\n",
    "print(\"Factors only in new encoding: \\n\", new_factors.difference(factor_list))\n",
    "print()\n",
    "print(\"--- Total number of encoded factors:\", len(all_factors_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Map the encoded factors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFactor(fac_type, transp, code):\n",
    "    temp_transp_str = trasp_mode_dict[int(transp)]\n",
    "\n",
    "    # if it is already a string, use it as factor\n",
    "    if type(code) == str:\n",
    "        fact_str = code\n",
    "\n",
    "    else:\n",
    "        try:  # check if it was in the first encoding\n",
    "            fact_str = exp_fact[fac_type][temp_transp_str][float(code)]\n",
    "        except:\n",
    "            try:  # check if it was in the new encoding\n",
    "                fact_str = new_fact[float(code)]\n",
    "            except:\n",
    "                fact_str = \"\"\n",
    "        return fact_str\n",
    "\n",
    "\n",
    "all_factors[\"factor\"] = all_factors.apply(\n",
    "    lambda x: getFactor(x[\"type\"], x[\"correctedModeOfTransport\"], x[\"code\"]), axis=1\n",
    ")\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Check not encoded factors**\n",
    "\n",
    "- mode of transport `unknown` and `otherPublic` are not present in the encodings\n",
    "- we have the encoding for the walking category from 0 to 8 but there is a single leg (#25:5856) with code 9 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_encoded_factors = all_factors[all_factors.factor == \"\"]\n",
    "print(\"legs without encoding:\", not_encoded_factors.shape)\n",
    "\n",
    "not_encoded_factors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "# all_factors.to_pickle(out_path + \"all_factors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**6. Merge factors**\n",
    "\n",
    "- Air_Quality_Temperature and Air_Quality --> Air_Quality\n",
    "- Predictability_Of_Travel_Time, Schedule_Reliability,  Reliability_Of_Travel_Time --> Reliability_Of_Travel_Time\n",
    "- Urban_Scenery_And_Atmosphere, Scenery, Nature_And_Scenery --> Scenery\n",
    "- Ability_To_Take_Kids_Or_Pets_Along , Ability_To_Take_Pets_Along --> Ability_To_Take_Kids_Or_Pets_Along\n",
    "- Accessibility_Escalators_Lifts_Ramps_Stairs_Etc , Convenient_Access_Lifts_Boarding --> Convenient_Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = pd.read_pickle(out_path + \"all_factors.pkl\")\n",
    "print(\"before preprocessing: \", all_factors.factor.nunique())\n",
    "# air quality\n",
    "all_factors[\"factor\"] = all_factors[\"factor\"].apply(\n",
    "    lambda x: \"Air_Quaility\" if x == \"Air_Quality_Temperature\" else x\n",
    ")\n",
    "\n",
    "# Reliability_Of_Travel_Time\n",
    "rel_lst = [\n",
    "    \"Predictability_Of_Travel_Time\",\n",
    "    \"Schedule_Reliability\",\n",
    "    \"Reliability_Of_Travel_Time\",\n",
    "]\n",
    "all_factors[\"factor\"] = all_factors[\"factor\"].apply(\n",
    "    lambda x: \"Reliability_Of_Travel_Time\" if x in rel_lst else x\n",
    ")\n",
    "\n",
    "# Scenery\n",
    "lst = [\"Urban_Scenery_And_Atmosphere\", \"Scenery\", \"Nature_And_Scenery\"]\n",
    "all_factors[\"factor\"] = all_factors[\"factor\"].apply(\n",
    "    lambda x: \"Scenery\" if x in lst else x\n",
    ")\n",
    "\n",
    "# Ability_To_Take_Kids_Or_Pets_Along\n",
    "all_factors[\"factor\"] = all_factors[\"factor\"].apply(\n",
    "    lambda x: \"Ability_To_Take_Kids_Or_Pets_Along\"\n",
    "    if x == \"Ability_To_Take_Pets_Along\"\n",
    "    else x\n",
    ")\n",
    "\n",
    "# Accessibility\n",
    "lst = [\n",
    "    \"Accessibility_Escalators_Lifts_Ramps_Stairs_Etc\",\n",
    "    \"Convenient_Access_Lifts_Boarding\",\n",
    "]\n",
    "all_factors[\"factor\"] = all_factors[\"factor\"].apply(\n",
    "    lambda x: \"Convenient_Access\" if x in lst else x\n",
    ")\n",
    "\n",
    "print(\"after preprocessing: \", all_factors.factor.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "all_factors.to_pickle(out_path + \"all_factors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Raw activities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "all_legs_merged = pd.read_pickle(out_path + \"all_legs_merged_1.pkl\")\n",
    "print(all_legs_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act = all_legs_merged[[\"tripid\", \"legid\", \"genericActivities\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_act(act_dict_list):\n",
    "    return len(act_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act[\"count_act\"] = raw_act[\"genericActivities\"].apply(count_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act.groupby(\"count_act\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act_clean = raw_act.loc[~raw_act[\"genericActivities\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_act_clean.loc[raw_act_clean[\"count_act\"] == 2].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_act_clean.loc[raw_act_clean[\"count_act\"] > 3].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_col = pd.DataFrame(\n",
    "    raw_act_clean[\"genericActivities\"].values.tolist(),\n",
    "    index=raw_act_clean.index,\n",
    "    columns=[\"activity_{:02d}\".format(i) for i in range(1, 10 + 1)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activities_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indexes = raw_act_clean.loc[raw_act_clean[\"count_act\"] > 3].head(5).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_col.loc[example_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_raw_act = raw_act[[\"tripid\", \"legid\", \"count_act\"]].join(activities_col)\n",
    "\n",
    "all_raw_act.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_activities_rows = (\n",
    "    all_raw_act.set_index([\"tripid\", \"legid\", \"count_act\"])\n",
    "    .stack()\n",
    "    .reset_index(name=\"activity_dict\")\n",
    "    .rename(columns={\"level_3\": \"activity_no\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_act_dict(actdict):\n",
    "    return actdict[\"text\"], actdict[\"code\"]\n",
    "\n",
    "\n",
    "(\n",
    "    all_raw_activities_rows[\"activity_text\"],\n",
    "    all_raw_activities_rows[\"activity_code\"],\n",
    ") = zip(*all_raw_activities_rows[\"activity_dict\"].apply(map_act_dict))\n",
    "\n",
    "all_raw_activities_rows.drop(\"activity_dict\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_raw_activities_rows.loc[all_raw_activities_rows[\"count_act\"] > 2].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "all_raw_activities_rows.to_pickle(out_path + \"all_raw_activities_rows.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "activity_labels = (\n",
    "    all_raw_activities_rows.groupby([\"activity_text\", \"activity_code\"])\n",
    "    .size()\n",
    "    .to_frame(\"size\")\n",
    "    .sort_values(by=\"size\", ascending=False)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "activity_labels.to_csv(out_path + \"activity_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='values_from_trip'></a>\n",
    "### values_from_trip\n",
    "\n",
    "1. Select values from trips\n",
    "2. Mapping the value to the Activity (Enjoyment, Fitness, Payed Work, Productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Select values from trips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values_from_trip = []\n",
    "first = True\n",
    "for index, leg in all_legs_merged.iterrows():\n",
    "    vft_curr = pd.DataFrame(list(leg[\"valueFromTrip\"]))\n",
    "    vft_curr[\"tripid\"] = leg[\"tripid\"]\n",
    "    vft_curr[\"legid\"] = leg[\"legid\"]\n",
    "    vft_curr[\"legStartDay\"] = leg[\"legStartDay\"]\n",
    "\n",
    "    if first:\n",
    "        all_values_from_trip = vft_curr\n",
    "        first = False\n",
    "    else:\n",
    "        all_values_from_trip = pd.concat([all_values_from_trip, vft_curr])\n",
    "\n",
    "print(all_values_from_trip.shape)\n",
    "all_values_from_trip.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Mapping the value to the Activity (Enjoyment, Fitness, Payed Work, Productivity)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the activity\n",
    "vft_dict = {\n",
    "    \"-1\": \"Unknown\",\n",
    "    \"0\": \"Paid_work\",\n",
    "    \"1\": \"Personal_tasks\",\n",
    "    \"2\": \"Enjoyment\",\n",
    "    \"3\": \"Fitness\",\n",
    "}\n",
    "all_values_from_trip[\"valueFromTrip\"] = all_values_from_trip[\"code\"].apply(\n",
    "    lambda x: vft_dict[str(int(x))]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Legs with values from trips:\",\n",
    "    len(all_values_from_trip[\"legid\"].unique()),\n",
    "    \" - \",\n",
    "    np.round(\n",
    "        len(all_values_from_trip[\"legid\"].unique())\n",
    "        / len(all_legs[\"legid\"].unique())\n",
    "        * 100,\n",
    "        2,\n",
    "    ),\n",
    "    \"%\",\n",
    ")\n",
    "print(\n",
    "    \"Trips with a values from trips:\",\n",
    "    len(all_values_from_trip[\"tripid\"].unique()),\n",
    "    \" - \",\n",
    "    np.round(\n",
    "        len(all_values_from_trip[\"tripid\"].unique())\n",
    "        / len(all_legs[\"tripid\"].unique())\n",
    "        * 100,\n",
    "        2,\n",
    "    ),\n",
    "    \"%\",\n",
    ")\n",
    "\n",
    "all_values_from_trip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "all_values_from_trip.to_pickle(out_path + \"values_from_trip.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outliers_detection'></a>\n",
    "### outliers_detection\n",
    "\n",
    "Check outliers for each mode of transport using the variable **`legDistance`**\n",
    "\n",
    "1. Create lower and upper bounds\n",
    "2. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "all_legs_merged = pd.read_pickle(out_path + \"all_legs_merged_1.pkl\")\n",
    "print(all_legs_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "variable = \"legDistance\"\n",
    "# sort all the legs according to the duration\n",
    "legs_in_multi_legs_trips = all_legs_merged.sort_values(\n",
    "    [\"tripid\", variable], ascending=[True, False]\n",
    ")\n",
    "# order for each trip the legs by duration\n",
    "legs_in_multi_legs_trips[\"rank\"] = (\n",
    "    legs_in_multi_legs_trips.groupby([\"tripid\"]).cumcount() + 1\n",
    ")\n",
    "# take the first leg -> the longest\n",
    "# should be one leg for each trip\n",
    "longest_legs = legs_in_multi_legs_trips[legs_in_multi_legs_trips[\"rank\"] == 1]\n",
    "print(\"Number of longest legs equal to number of trips\")\n",
    "print(\"trips:\", len(legs_in_multi_legs_trips.tripid.unique()))\n",
    "print(\"longest legs:\", len(longest_legs))\n",
    "print()\n",
    "\n",
    "### Use the quantile method\n",
    "# remove the observations having the lowest and the highest values\n",
    "quant = 0.01\n",
    "low_quant = quant\n",
    "up_quant = 1 - quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create lower and upper bounds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the upper bounds of distance and duration for each mode of transport\n",
    "stats_uptime = (\n",
    "    longest_legs.groupby(\"correctedModeOfTransport_str\")[\n",
    "        \"inferred_leg_duration_min\", \"legDistance\"\n",
    "    ]\n",
    "    .quantile(up_quant)\n",
    "    .reset_index()\n",
    ")\n",
    "stats_uptime.columns = [\n",
    "    \"correctedModeOfTransport_str\",\n",
    "    \"up_bound_time\",\n",
    "    \"up_bound_dist\",\n",
    "]\n",
    "\n",
    "# create the lower bounds of distance and duration for each mode of transport\n",
    "stats_lowtime = (\n",
    "    longest_legs.groupby(\"correctedModeOfTransport_str\")[\n",
    "        \"inferred_leg_duration_min\", \"legDistance\"\n",
    "    ]\n",
    "    .quantile(low_quant)\n",
    "    .reset_index()\n",
    ")\n",
    "stats_lowtime.columns = [\n",
    "    \"correctedModeOfTransport_str\",\n",
    "    \"low_bound_time\",\n",
    "    \"low_bound_dist\",\n",
    "]\n",
    "\n",
    "# merge and add a count\n",
    "stats = pd.merge(\n",
    "    stats_lowtime, stats_uptime, on=\"correctedModeOfTransport_str\", how=\"left\"\n",
    ")\n",
    "stats = pd.merge(\n",
    "    stats_lowtime, stats_uptime, on=\"correctedModeOfTransport_str\", how=\"left\"\n",
    ")\n",
    "count_stat = longest_legs.groupby(\"correctedModeOfTransport_str\").size().reset_index()\n",
    "count_stat.columns = [\"correctedModeOfTransport_str\", \"count\"]\n",
    "stats = pd.merge(count_stat, stats, on=\"correctedModeOfTransport_str\", how=\"left\")\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Remove outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_legs_merged_no_outlier_no_outlier = pd.merge(\n",
    "    all_legs_merged_no_outlier, stats, on=\"correctedModeOfTransport_str\", how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Initial number of legs: \", all_legs_merged_no_outlier_no_outlier.shape[0])\n",
    "all_legs_merged_no_outlier = all_legs_merged_no_outlier[\n",
    "    (\n",
    "        (\n",
    "            all_legs_merged_no_outlier[\"legDistance\"]\n",
    "            >= all_legs_merged_no_outlier[\"low_bound_dist\"]\n",
    "        )\n",
    "        & (\n",
    "            all_legs_merged_no_outlier[\"legDistance\"]\n",
    "            <= all_legs_merged_no_outlier[\"up_bound_dist\"]\n",
    "        )\n",
    "        & (\n",
    "            all_legs_merged_no_outlier[\"inferred_leg_duration_min\"]\n",
    "            >= all_legs_merged_no_outlier[\"low_bound_time\"]\n",
    "        )\n",
    "        & (\n",
    "            all_legs_merged_no_outlier[\"inferred_leg_duration_min\"]\n",
    "            <= all_legs_merged_no_outlier[\"up_bound_time\"]\n",
    "        )\n",
    "    )\n",
    "    | (all_legs_merged_no_outlier[\"class\"] != \"Leg\")\n",
    "]\n",
    "print(\"Legs without outliers: \", all_legs_merged_no_outlier.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE\n",
    "all_legs_merged_no_outlier.to_pickle(\n",
    "    out_path + \"all_legs_merged_no_outlier_\" + str(quant) + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
